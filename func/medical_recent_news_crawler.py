#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìµœê·¼ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ì˜¤ëŠ˜/ì–´ì œ ë‰´ìŠ¤ë§Œ í•„í„°ë§)
XPathë¥¼ ì‚¬ìš©í•˜ì—¬ ë‚ ì§œ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ë¥¼ í•„í„°ë§í•˜ê³  ìƒì„¸ ì •ë³´ë¥¼ í¬ë¡¤ë§
"""

import json
import time
from datetime import datetime, timedelta
import os
import tempfile
import uuid
import random
import subprocess
import re
import requests
from bs4 import BeautifulSoup
from dateutil import parser
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import openai
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

def setup_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì„¤ì • (Docker í™˜ê²½ ìµœì í™”)"""
    
    # ê¸°ì¡´ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
    try:
        subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
        subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
        time.sleep(1)
        print("ğŸ§¹ ê¸°ì¡´ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass
    
    try:
        chrome_options = Options()
        
        # Docker í™˜ê²½ í•„ìˆ˜ ì˜µì…˜
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-software-rasterizer')
        chrome_options.add_argument('--window-size=1280,720')
        
        # SSL ë° ë³´ì•ˆ ê´€ë ¨ ì˜µì…˜
        chrome_options.add_argument('--ignore-ssl-errors-on-quic')
        chrome_options.add_argument('--ignore-certificate-errors')
        chrome_options.add_argument('--ignore-ssl-errors')
        chrome_options.add_argument('--ignore-certificate-errors-spki-list')
        chrome_options.add_argument('--disable-extensions-file-access-check')
        chrome_options.add_argument('--allow-running-insecure-content')
        
        # ì„¸ì…˜ ì¶©ëŒ ë°©ì§€ ë° ì•ˆì •ì„± ì˜µì…˜
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor,ChromeWhatsNewUI')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--disable-background-timer-throttling')
        chrome_options.add_argument('--disable-renderer-backgrounding')
        chrome_options.add_argument('--disable-backgrounding-occluded-windows')
        chrome_options.add_argument('--disable-client-side-phishing-detection')
        chrome_options.add_argument('--disable-component-extensions-with-background-pages')
        chrome_options.add_argument('--disable-ipc-flooding-protection')
        chrome_options.add_argument('--no-default-browser-check')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-background-networking')
        
        # ê³ ìœ  ì„¸ì…˜ ìƒì„±
        temp_dir = tempfile.mkdtemp(prefix=f'recent_news_chrome_{uuid.uuid4().hex[:8]}_')
        chrome_options.add_argument(f'--user-data-dir={temp_dir}')
        
        debug_port = random.randint(9500, 9999)
        chrome_options.add_argument(f'--remote-debugging-port={debug_port}')
        
        # ì„±ëŠ¥ ìµœì í™”
        chrome_options.add_argument('--memory-pressure-off')
        chrome_options.add_argument('--max_old_space_size=2048')
        chrome_options.add_argument('--aggressive-cache-discard')
        
        # User Agent ì„¤ì • (ëª¨ë°”ì¼)
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1')
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        print(f"ğŸ”§ Chrome ì„¸ì…˜ ë””ë ‰í† ë¦¬: {temp_dir}")
        print(f"ğŸ”§ ë””ë²„ê¹… í¬íŠ¸: {debug_port}")
        
        # Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” - Docker í™˜ê²½ì—ì„œëŠ” ì‹œìŠ¤í…œ ChromeDriver ì‚¬ìš©
        driver = None
        try:
            # Docker í™˜ê²½ì—ì„œëŠ” ì‹œìŠ¤í…œ ChromeDriver ì§ì ‘ ì‚¬ìš© (ì•„í‚¤í…ì²˜ í˜¸í™˜ì„± ë¬¸ì œ ë°©ì§€)
            driver = webdriver.Chrome(options=chrome_options)
            
        except Exception as e:
            print(f"âŒ Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
        
        if driver:
            # íƒ€ì„ì•„ì›ƒ ë° ê¸°ë³¸ ì„¤ì •
            driver.implicitly_wait(10)
            driver.set_page_load_timeout(30)
            
            # ì„¸ì…˜ ì •ë³´ ì €ì¥ (ì •ë¦¬ìš©)
            driver._temp_dir = temp_dir
            driver._debug_port = debug_port
            
            # ìë™í™” ê°ì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£Œ")
            return driver
        
    except Exception as e:
        print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
        return None

def parse_date(date_str):
    """ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
    try:
        # "2025.08.08" í˜•íƒœì˜ ë‚ ì§œ ë¬¸ìì—´ì„ ì²˜ë¦¬
        if "." in date_str:
            return datetime.strptime(date_str.strip(), '%Y.%m.%d')
        # ë‹¤ë¥¸ í˜•íƒœì˜ ë‚ ì§œ ì²˜ë¦¬ ê°€ëŠ¥ì„± ê³ ë ¤
        return None
    except Exception as e:
        print(f"âŒ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {e}")
        return None

def is_recent_news(date_str, target_dates):
    """ë‰´ìŠ¤ ë‚ ì§œê°€ ì˜¤ëŠ˜ ë˜ëŠ” ì–´ì œì¸ì§€ í™•ì¸"""
    news_date = parse_date(date_str)
    if news_date is None:
        return False
    
    # ë‚ ì§œë§Œ ë¹„êµ (ì‹œê°„ ì œì™¸)
    news_date_only = news_date.date()
    return news_date_only in target_dates

def collect_recent_news_urls(driver, target_dates=None):
    """ìµœê·¼ ë‰´ìŠ¤ URL ìˆ˜ì§‘"""
    print("ğŸ” ìµœê·¼ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì‹œì‘...")
    
    # ëŒ€ìƒ ë‚ ì§œ ì„¤ì •
    if target_dates is None:
        # TEST_DATE í™˜ê²½ë³€ìˆ˜ í™•ì¸
        import os
        test_date_env = os.getenv('TEST_DATE')
        if test_date_env:
            try:
                # TEST_DATE íŒŒì‹± (í˜•ì‹: 'YYYY-MM-DD')
                test_date = datetime.strptime(test_date_env, '%Y-%m-%d').date()
                yesterday = test_date - timedelta(days=1)
                target_dates = {test_date, yesterday}
                print(f"ğŸ¯ TEST_DATE ì‚¬ìš©: {test_date.strftime('%Y.%m.%d')} (ì§€ì •ì¼), {yesterday.strftime('%Y.%m.%d')} (ì „ì¼)")
            except ValueError:
                print(f"âŒ ì˜ëª»ëœ TEST_DATE í˜•ì‹: {test_date_env}, í˜„ì¬ ë‚ ì§œ ì‚¬ìš©")
                # ê¸°ë³¸ê°’: ì˜¤ëŠ˜ê³¼ ì–´ì œ ë‚ ì§œ
                today = datetime.now().date()
                yesterday = today - timedelta(days=1)
                target_dates = {today, yesterday}
                print(f"ğŸ¯ ê¸°ë³¸ ëŒ€ìƒ ë‚ ì§œ: {today.strftime('%Y.%m.%d')} (ì˜¤ëŠ˜), {yesterday.strftime('%Y.%m.%d')} (ì–´ì œ)")
        else:
            # ê¸°ë³¸ê°’: ì˜¤ëŠ˜ê³¼ ì–´ì œ ë‚ ì§œ
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            target_dates = {today, yesterday}
            print(f"ğŸ¯ ê¸°ë³¸ ëŒ€ìƒ ë‚ ì§œ: {today.strftime('%Y.%m.%d')} (ì˜¤ëŠ˜), {yesterday.strftime('%Y.%m.%d')} (ì–´ì œ)")
    else:
        # ì‚¬ìš©ì ì§€ì • ë‚ ì§œ
        date_strings = [date.strftime('%Y.%m.%d') for date in target_dates]
        print(f"ğŸ¯ ì‚¬ìš©ì ì§€ì • ë‚ ì§œ: {', '.join(date_strings)}")
    
    news_urls = []
    
    # í˜ì´ì§€ êµ¬ì¡° í™•ì¸ ë° ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì°¾ê¸°
    print("ğŸ“¦ í˜ì´ì§€ êµ¬ì¡° í™•ì¸ ì¤‘...")
    
    try:
        # main_con ìš”ì†Œ ì§ì ‘ ì°¾ê¸° (ê¸°ì¡´ í¬ë¡¤ëŸ¬ì™€ ë™ì¼í•˜ê²Œ)
        main_con_xpath = "//*[@id='main_con']/div[1]/div/div[1]/ul"
        print(f"ğŸ“¦ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ê²€ìƒ‰: {main_con_xpath}")
        
        # main_con ìš”ì†Œ ëŒ€ê¸° (ë” ì˜¤ë˜)
        print("ğŸ” main_con ìš”ì†Œ ëŒ€ê¸° ì¤‘...")
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.ID, "main_con"))
            )
            print("âœ… main_con ë°œê²¬")
        except Exception as debug_e:
            print(f"âŒ main_con ì°¾ê¸° ì‹¤íŒ¨: {debug_e}")
            # ì¶”ê°€ ëŒ€ê¸° í›„ ì¬ì‹œë„
            print("â° ì¶”ê°€ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
            time.sleep(10)
            try:
                driver.find_element(By.ID, "main_con")
                print("âœ… main_con ì¬ì‹œë„ ì„±ê³µ")
            except:
                print("âŒ main_con ì¬ì‹œë„ë„ ì‹¤íŒ¨")
        
        news_list = WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.XPATH, main_con_xpath))
        )
        print("âœ… ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë°œê²¬")
        
        # li ìš”ì†Œë“¤ ì°¾ê¸°
        li_elements = news_list.find_elements(By.TAG_NAME, "li")
        print(f"ğŸ“„ li ìš”ì†Œ {len(li_elements)}ê°œ ë°œê²¬")
        
        for li_idx, li_element in enumerate(li_elements, 1):
            try:
                # ë‚ ì§œ í™•ì¸ (ì‚¬ìš©ìê°€ ì œì‹œí•œ ê²½ë¡œ: li[n]/a/table/tbody/tr/td[2]/div[2]/span/text()[2])
                news_date = None
                try:
                    # div[2] ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
                    date_div_xpath = ".//a/table/tbody/tr/td[2]/div[2]"
                    date_div = li_element.find_element(By.XPATH, date_div_xpath)
                    date_text = date_div.text.strip()
                    
                    # YYYY.MM.DD íŒ¨í„´ ì°¾ê¸°
                    import re
                    date_match = re.search(r'\d{4}\.\d{2}\.\d{2}', date_text)
                    if date_match:
                        news_date = date_match.group()
                except Exception as date_e:
                    print(f"         âŒ ë‚ ì§œ div ì°¾ê¸° ì‹¤íŒ¨: {date_e}")
                    
                    # spanì—ì„œ ì§ì ‘ ì°¾ê¸° ì‹œë„
                    try:
                        date_spans = li_element.find_elements(By.XPATH, ".//a/table/tbody/tr/td[2]/div[2]/span")
                        for span in date_spans:
                            text_content = span.text.strip()
                            if re.match(r'\d{4}\.\d{2}\.\d{2}', text_content):
                                news_date = text_content
                                break
                    except:
                        pass
                
                if not news_date:
                    print(f"     âŒ li[{li_idx}]ì—ì„œ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    continue
                
                print(f"     ğŸ“… li[{li_idx}] ë‚ ì§œ: {news_date}")
                
                # ì›í•˜ëŠ” ë‚ ì§œì¸ì§€ í™•ì¸ (íš¨ìœ¨ì„± ê°œì„ )
                if is_recent_news(news_date, target_dates):
                    print(f"     âœ… ëŒ€ìƒ ë‰´ìŠ¤ ë°œê²¬! ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
                    
                    # ë§í¬ URL ê°€ì ¸ì˜¤ê¸° (li[n]/a)
                    try:
                        link_element = li_element.find_element(By.XPATH, ".//a")
                        news_url = link_element.get_attribute('href')
                        
                        # ì œëª© ë¯¸ë¦¬ë³´ê¸° ê°€ì ¸ì˜¤ê¸° (li[n]/a/table/tbody/tr/td[2]/div[1])
                        title_preview = ""
                        try:
                            title_element = li_element.find_element(By.XPATH, ".//a/table/tbody/tr/td[2]/div[1]")
                            title_preview = title_element.text.strip()[:50]
                        except:
                            title_preview = "ì œëª© ë¯¸ë¦¬ë³´ê¸° ì—†ìŒ"
                        
                        if news_url:
                            # ì¤‘ë³µ ì²´í¬
                            duplicate = False
                            for existing in news_urls:
                                if existing['url'] == news_url:
                                    duplicate = True
                                    break
                            
                            if not duplicate:
                                news_urls.append({
                                    'url': news_url,
                                    'date': news_date,
                                    'title_preview': title_preview,
                                    'li_index': li_idx
                                })
                                print(f"         ğŸ“° {title_preview}...")
                                print(f"         ğŸ”— {news_url}")
                            else:
                                print(f"         âš ï¸ ì¤‘ë³µ URL ê±´ë„ˆëœ€")
                    except Exception as e:
                        print(f"         âŒ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                else:
                    print(f"     â­ï¸ ëŒ€ìƒ ì™¸ ë‚ ì§œ, ê±´ë„ˆëœ€: {news_date}")
                    
            except Exception as e:
                print(f"     âŒ li[{li_idx}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
                
    except TimeoutException:
        print("âŒ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    except Exception as e:
        print(f"âŒ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    print(f"ğŸ¯ ì´ {len(news_urls)}ê°œì˜ ìµœê·¼ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì™„ë£Œ")
    return news_urls

def extract_article_date_recent(driver):
    """ìµœê·¼ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ìš© ê¸°ì‚¬ì—ì„œ ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ (ì§ì ‘ XPath ì‚¬ìš©)"""
    try:
        # ì‚¬ìš©ìê°€ ì œê³µí•œ ì •í™•í•œ XPath ì‚¬ìš©
        date_xpath = "//*[@id='main_con']/div[1]/div/div[1]/div[1]/div[2]/div[2]"
        
        try:
            date_element = WebDriverWait(driver, 3).until(
                EC.presence_of_element_located((By.XPATH, date_xpath))
            )
            date_text = date_element.text.strip()
            
            if date_text:
                print(f"    ğŸ” ì—…ë¡œë“œ ë‚ ì§œ ë°œê²¬: {date_text}")
                return parse_and_format_date_recent(date_text)
            else:
                print("    âš ï¸ ë‚ ì§œ ìš”ì†ŒëŠ” ìˆì§€ë§Œ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
                
        except TimeoutException:
            print(f"    âš ï¸ ì§€ì •ëœ XPathì—ì„œ ë‚ ì§œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {date_xpath}")
        except Exception as e:
            print(f"    âŒ ë‚ ì§œ ìš”ì†Œ ì ‘ê·¼ ì˜¤ë¥˜: {e}")
        
        # ë°±ì—…: ë‹¤ë¥¸ ê°€ëŠ¥í•œ ìœ„ì¹˜ë“¤ ë¹ ë¥´ê²Œ í™•ì¸
        backup_selectors = [
            "//*[@id='main_con']/div[1]/div/div[1]/div[2]/div[1]/span",
            "//*[@id='main_con']/div[1]/div/div[1]/div[2]/div[1]",
            "//*[@id='main_con']/div[1]/div/div[1]/div[2]/span"
        ]
        
        for selector in backup_selectors:
            try:
                elem = driver.find_element(By.XPATH, selector)
                text = elem.text.strip()
                if text and (re.search(r'\d{4}', text) or 'ì‹œê°„ ì „' in text or 'ë¶„ ì „' in text):
                    print(f"    ğŸ” ë°±ì—…ìœ¼ë¡œ ë‚ ì§œ ë°œê²¬: {selector} -> {text}")
                    return parse_and_format_date_recent(text)
            except:
                continue
        
        print("    âš ï¸ ëª¨ë“  ì‹œë„ì—ì„œ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None
        
    except Exception as e:
        print(f"    âŒ ë‚ ì§œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

def parse_and_format_date_recent(date_text):
    """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±í•˜ì—¬ YYYY.MM.DD hh:mm í˜•íƒœë¡œ ë³€í™˜"""
    try:
        current_time = datetime.now()
        
        print(f"    ğŸ” ë‚ ì§œ íŒŒì‹± ì‹œë„: '{date_text}'")
        
        # "ì…ë ¥"ê³¼ "ìˆ˜ì •" ë‚ ì§œê°€ í•¨ê»˜ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if 'ì…ë ¥' in date_text:
            # "ì…ë ¥ 2025-08-12 09:34 ìˆ˜ì • 2025.08.12 09:34" ì—ì„œ ì…ë ¥ ë‚ ì§œë§Œ ì¶”ì¶œ
            input_match = re.search(r'ì…ë ¥[^\d]*(\d{4}[-.\s]\d{1,2}[-.\s]\d{1,2}[^\d]*\d{1,2}:\d{2})', date_text)
            if input_match:
                input_date = input_match.group(1).strip()
                print(f"    âœ… ì…ë ¥ ë‚ ì§œ ì¶”ì¶œ: '{input_date}'")
                # í•˜ì´í”ˆì„ ì ìœ¼ë¡œ ë³€í™˜í•˜ê³  ê³µë°± ì œê±°
                input_date = re.sub(r'[-\s]', '.', input_date)
                input_date = re.sub(r'\.+', '.', input_date)  # ì—°ì†ëœ ì  ì œê±°
                # ì¬ê·€ í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ì²˜ë¦¬
                try:
                    dt = parser.parse(input_date, fuzzy=True)
                    return dt.strftime("%Y.%m.%d %H:%M")
                except:
                    # ìˆ˜ë™ íŒŒì‹±
                    parts = input_date.split()
                    if len(parts) >= 2:
                        date_part = parts[0].replace('.', '-')
                        time_part = parts[1]
                        try:
                            dt = datetime.strptime(f"{date_part} {time_part}", "%Y-%m-%d %H:%M")
                            return dt.strftime("%Y.%m.%d %H:%M")
                        except:
                            pass
                    return current_time.strftime("%Y.%m.%d %H:%M")
            
            # ì‹œê°„ì´ ì—†ëŠ” ê²½ìš°: "ì…ë ¥ 2025-08-12"
            input_match_no_time = re.search(r'ì…ë ¥[^\d]*(\d{4}[-.\s]\d{1,2}[-.\s]\d{1,2})', date_text)
            if input_match_no_time:
                input_date = input_match_no_time.group(1).strip() + " 00:00"
                print(f"    âœ… ì…ë ¥ ë‚ ì§œ ì¶”ì¶œ (ì‹œê°„ ì—†ìŒ): '{input_date}'")
                input_date = re.sub(r'[-\s]', '.', input_date)
                # ì¬ê·€ í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ì²˜ë¦¬
                try:
                    dt = parser.parse(input_date, fuzzy=True)
                    return dt.strftime("%Y.%m.%d %H:%M")
                except:
                    return current_time.strftime("%Y.%m.%d %H:%M")
        
        # "ì‘ì„±"ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (SBSì™€ ìœ ì‚¬í•œ íŒ¨í„´)
        elif 'ì‘ì„±' in date_text:
            created_match = re.search(r'ì‘ì„±[^\d]*(\d{4}[-.\s]\d{1,2}[-.\s]\d{1,2}[^\d]*\d{1,2}:\d{2})', date_text)
            if created_match:
                created_date = created_match.group(1).strip()
                print(f"    âœ… ì‘ì„± ë‚ ì§œ ì¶”ì¶œ: '{created_date}'")
                created_date = re.sub(r'[-\s]', '.', created_date)
                created_date = re.sub(r'\.+', '.', created_date)
                # ì¬ê·€ í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ì²˜ë¦¬
                try:
                    dt = parser.parse(created_date, fuzzy=True)
                    return dt.strftime("%Y.%m.%d %H:%M")
                except:
                    return current_time.strftime("%Y.%m.%d %H:%M")
        
        # ìƒëŒ€ì  ì‹œê°„ í‘œí˜„ ì²˜ë¦¬
        elif 'ì‹œê°„ ì „' in date_text:
            hours_ago = int(re.search(r'(\d+)ì‹œê°„', date_text).group(1))
            target_time = current_time - timedelta(hours=hours_ago)
            return target_time.strftime("%Y.%m.%d %H:%M")
        
        elif 'ë¶„ ì „' in date_text:
            minutes_ago = int(re.search(r'(\d+)ë¶„', date_text).group(1))
            target_time = current_time - timedelta(minutes=minutes_ago)
            return target_time.strftime("%Y.%m.%d %H:%M")
        
        elif 'ì¼ ì „' in date_text:
            days_ago = int(re.search(r'(\d+)ì¼', date_text).group(1))
            target_time = current_time - timedelta(days=days_ago)
            return target_time.strftime("%Y.%m.%d %H:%M")
        
        # í•˜ì´í”ˆ í˜•ì‹ ë‚ ì§œ ì²˜ë¦¬: "2025-08-12 09:34" ë“±
        elif re.search(r'\d{4}-\d{1,2}-\d{1,2}', date_text):
            try:
                # ì‹œê°„ì´ ìˆëŠ” ê²½ìš°: "2025-08-12 09:34"
                datetime_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})\s+(\d{1,2}):(\d{2})', date_text)
                if datetime_match:
                    year, month, day, hour, minute = datetime_match.groups()
                    result = f"{year}.{month.zfill(2)}.{day.zfill(2)} {hour.zfill(2)}:{minute}"
                    print(f"    âœ… í•˜ì´í”ˆ í˜•ì‹ ë‚ ì§œ íŒŒì‹±: '{result}'")
                    return result
                
                # ë‚ ì§œë§Œ ìˆëŠ” ê²½ìš°: "2025-08-12"
                date_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_text)
                if date_match:
                    year, month, day = date_match.groups()
                    result = f"{year}.{month.zfill(2)}.{day.zfill(2)} 00:00"
                    print(f"    âœ… í•˜ì´í”ˆ í˜•ì‹ ë‚ ì§œ íŒŒì‹± (ì‹œê°„ ì—†ìŒ): '{result}'")
                    return result
                    
            except Exception as e:
                print(f"    âŒ í•˜ì´í”ˆ í˜•ì‹ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        # ì•½ì—…ë‹·ì»´ íŠ¹ìˆ˜ í˜•ì‹: "2024. 1. 15. 14:30"
        elif re.match(r'\d{4}\. \d{1,2}\. \d{1,2}\. \d{1,2}:\d{2}', date_text):
            # "2024. 1. 15. 14:30" -> "2024.01.15 14:30"
            parts = date_text.split('. ')
            if len(parts) >= 4:
                year = parts[0]
                month = parts[1].zfill(2)
                day = parts[2].zfill(2)
                time_part = parts[3] if ':' in parts[3] else "00:00"
                return f"{year}.{month}.{day} {time_part}"
        
        # í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        elif 'ë…„' in date_text and 'ì›”' in date_text and 'ì¼' in date_text:
            date_match = re.search(r'(\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼\s*(\d{1,2}):(\d{2})', date_text)
            if date_match:
                year, month, day, hour, minute = date_match.groups()
                return f"{year}.{month.zfill(2)}.{day.zfill(2)} {hour.zfill(2)}:{minute}"
        
        # ì¼ë°˜ì ì¸ ë‚ ì§œ í˜•ì‹ë“¤ ì‹œë„
        date_formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d %H:%M',
            '%Y.%m.%d %H:%M',
            '%Y/%m/%d %H:%M',
            '%Y-%m-%d',
            '%Y.%m.%d',
            '%Y/%m/%d'
        ]
        
        for fmt in date_formats:
            try:
                dt = datetime.strptime(date_text.strip(), fmt)
                return dt.strftime("%Y.%m.%d %H:%M")
            except ValueError:
                continue
        
        # dateutil.parser ì‚¬ìš© (ë§ˆì§€ë§‰ ì‹œë„)
        try:
            dt = parser.parse(date_text, fuzzy=True)
            return dt.strftime("%Y.%m.%d %H:%M")
        except:
            pass
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, í˜„ì¬ ì‹œê°„ ì‚¬ìš©: '{date_text}'")
        return current_time.strftime("%Y.%m.%d %H:%M")
        
    except Exception as e:
        print(f"    âŒ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return datetime.now().strftime("%Y.%m.%d %H:%M")

def crawl_news_detail(driver, news_item, rank):
    """ê°œë³„ ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
    news_url = news_item['url']
    print(f"ğŸ“° [{rank}] ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì¤‘...")
    print(f"   ğŸ“… ë‚ ì§œ: {news_item['date']}")
    print(f"   ğŸŒ URL: {news_url}")
    
    try:
        # í•´ë‹¹ URLë¡œ ì´ë™
        driver.get(news_url)
        time.sleep(2)
        
        news_info = {
            'rank': rank,
            'title': '',
            'content': '',
            'summary': '',  # AI ìš”ì•½ ì¶”ê°€
            'url': news_url,
            'date': news_item['date'],
            'pub_time': None,  # ì—…ë¡œë“œ ë‚ ì§œ/ì‹œê°„
            'li_index': news_item['li_index'],
            'title_preview': news_item['title_preview'],
            'type': 'medical news'
        }
        
        # ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ
        print("   ğŸ“… ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ ì¤‘...")
        pub_time = extract_article_date_recent(driver)
        if pub_time:
            news_info['pub_time'] = pub_time
            print(f"   âœ… ì—…ë¡œë“œ ë‚ ì§œ: {pub_time}")
        else:
            news_info['pub_time'] = datetime.now().strftime("%Y.%m.%d %H:%M")
            print("   âš ï¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨, í˜„ì¬ ì‹œê°„ ì‚¬ìš©")
        
        # ì œëª© ì¶”ì¶œ (ê¸°ì¡´ í¬ë¡¤ëŸ¬ì™€ ë™ì¼)
        try:
            title_xpath = "//*[@id='main_con']/div[1]/div/div[1]/div[1]"
            title_element = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.XPATH, title_xpath))
            )
            news_info['title'] = title_element.text.strip()
            print(f"   âœ… ì œëª©: {news_info['title']}")
        except Exception as e:
            print(f"   âŒ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            news_info['title'] = "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        
        # ë‚´ìš© ì¶”ì¶œ (ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¥¸ XPath)
        try:
            content_xpath = "//*[@id='main_con']/div[1]/div/div[1]/div[2]/div[2]"
            content_element = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.XPATH, content_xpath))
            )
            news_info['content'] = content_element.text.strip()
            print(f"   âœ… ë‚´ìš©: {len(news_info['content'])}ì ì¶”ì¶œ")
        except Exception as e:
            print(f"   âŒ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            news_info['content'] = "ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"
        
        # AI ìš”ì•½ ìƒì„±
        if news_info['content'] and news_info['content'] != "ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨":
            print("   ğŸ¤– AI ìš”ì•½ ìƒì„± ì¤‘...")
            summary = summarize_with_gpt4o(news_info['title'], news_info['content'])
            if summary:
                news_info['summary'] = summary
                print(f"   âœ… AI ìš”ì•½: {summary[:50]}...")
            else:
                news_info['summary'] = "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"
                print("   âŒ AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
        else:
            news_info['summary'] = "ë³¸ë¬¸ ë‚´ìš© ë¶€ì¡±"
            print("   âŒ ë³¸ë¬¸ ë‚´ìš©ì´ ë¶€ì¡±í•˜ì—¬ ìš”ì•½ ë¶ˆê°€")
        
        return news_info
        
    except Exception as e:
        print(f"   âŒ ë‰´ìŠ¤ [{rank}] ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return None

def should_filter_by_time_recent():
    """í˜„ì¬ ì‹œê°„ì— ë”°ë¥¸ ë‰´ìŠ¤ í•„í„°ë§ ì—¬ë¶€ ê²°ì •"""
    current_hour = datetime.now().hour
    
    if current_hour >= 13:
        print(f"â° í˜„ì¬ ì‹œê°: {current_hour}ì‹œ - 09ì‹œ ì´í›„ ë‰´ìŠ¤ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        return True
    else:
        print(f"â° í˜„ì¬ ì‹œê°: {current_hour}ì‹œ - ëª¨ë“  ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        return False

def is_news_time_valid_recent(news_pub_time, filter_enabled):
    """ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ì´ í•„í„°ë§ ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸"""
    if not filter_enabled or not news_pub_time:
        return True
    
    try:
        # YYYY.MM.DD hh:mm í˜•ì‹ íŒŒì‹±
        news_datetime = datetime.strptime(news_pub_time, "%Y.%m.%d %H:%M")
        today = datetime.now().date()
        
        # ì˜¤ëŠ˜ ë‚ ì§œì¸ ê²½ìš°ë§Œ ì‹œê°„ í•„í„° ì ìš©
        if news_datetime.date() == today:
            if news_datetime.hour >= 9:
                print(f"    âœ… ì‹œê°„ í•„í„° í†µê³¼: {news_pub_time} (09ì‹œ ì´í›„)")
                return True
            else:
                print(f"    âŒ ì‹œê°„ í•„í„° ì œì™¸: {news_pub_time} (09ì‹œ ì´ì „)")
                return False
        else:
            # ì˜¤ëŠ˜ì´ ì•„ë‹Œ ë‚ ì§œëŠ” ëª¨ë‘ í¬í•¨
            print(f"    âœ… ë‚ ì§œ í•„í„° í†µê³¼: {news_pub_time} (ì˜¤ëŠ˜ì´ ì•„ë‹Œ ë‚ ì§œ)")
            return True
            
    except Exception as e:
        print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜, ë‰´ìŠ¤ í¬í•¨: {news_pub_time} - {e}")
        return True

def summarize_with_gpt4o(title, content):
    """GPT-4oë¥¼ ì‚¬ìš©í•œ ê¸°ì‚¬ ìš”ì•½"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not os.getenv('OPENAI_API_KEY'):
            print("    âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return None
        
        if not content or len(content.strip()) < 50 or "ì¶”ì¶œ ì‹¤íŒ¨" in content:
            return "ë³¸ë¬¸ ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸° (GPT-4oëŠ” ë” ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥)
        if len(content) > 8000:
            content = content[:8000] + "..."
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„± (í•œêµ­ì–´ ì˜ë£Œ/ì œì•½ ë‰´ìŠ¤ì— íŠ¹í™”)
        prompt = f"""ë‹¤ìŒ ì˜ë£Œ/ì œì•½ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 3-4ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì œëª©: {title}

ë‚´ìš©: {content}

ìš”ì•½ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
1. í•µì‹¬ ë‚´ìš©ê³¼ ì£¼ìš” ìˆ˜ì¹˜ë¥¼ í¬í•¨í•˜ì„¸ìš”
2. ì˜ë£Œì§„, í™˜ì, ì œì•½ì—…ê³„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì–¸ê¸‰í•˜ì„¸ìš”
3. ê°ê´€ì ì´ê³  ì¤‘ë¦½ì ì¸ ì–´ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”

ìš”ì•½:"""
        
        # OpenAI API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=300
        )
        
        summary = response.choices[0].message.content.strip()
        return summary
        
    except Exception as e:
        print(f"    âŒ GPT-4o ìš”ì•½ ì˜¤ë¥˜: {e}")
        return None

def crawl_recent_news(base_url=None, target_dates=None):
    """ìµœê·¼ ë‰´ìŠ¤ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
    if base_url is None:
        base_url = "http://m.yakup.com/news/index.html?cat=11"  
    
    print("=" * 80)
    if target_dates is None:
        print("ğŸ“° ìµœê·¼ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ì˜¤ëŠ˜/ì–´ì œ ë‰´ìŠ¤)")
    else:
        date_strings = [date.strftime('%Y.%m.%d') for date in target_dates]
        print(f"ğŸ“° ìµœê·¼ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ({', '.join(date_strings)})")
    print("=" * 80)
    print(f"ğŸŒ ì ‘ì† URL: {base_url}")
    
    driver = None
    news_data = []
    
    try:
        # Chrome ë“œë¼ì´ë²„ ì„¤ì •
        driver = setup_chrome_driver()
        if not driver:
            raise Exception("Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨")
        
        print("ğŸ“± ë©”ì¸ í˜ì´ì§€ ë¡œë”© ì¤‘...")
        driver.get(base_url)
        
        # ê¸°ë³¸ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        print("âœ… ë©”ì¸ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
        
        # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° (ë” ê¸´ ì‹œê°„)
        print("â° ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸° ì¤‘...")
        time.sleep(8)
        
        # 1ë‹¨ê³„: ìµœê·¼ ë‰´ìŠ¤ URL ìˆ˜ì§‘ (ë‚ ì§œ í•„í„°ë§)
        recent_news = collect_recent_news_urls(driver, target_dates)
        
        if not recent_news:
            print("âš ï¸ ìˆ˜ì§‘ëœ ìµœê·¼ ë‰´ìŠ¤ URLì´ ì—†ìŠµë‹ˆë‹¤")
            return {
                'base_url': base_url,
                'status': 'ì„±ê³µ',
                'error': None,
                'news_count': 0,
                'news_list': [],
                'crawling_time': datetime.now().isoformat()
            }
        
        # 2ë‹¨ê³„: ê° ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        print(f"\nğŸ“Š {len(recent_news)}ê°œ ìµœê·¼ ë‰´ìŠ¤ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")
        
        i = 1
        while i <= len(recent_news):
            news_item = recent_news[i-1]
            
            # 10ê°œë§ˆë‹¤ ë˜ëŠ” ì²« ì‹œì‘ ì‹œ ChromeDriver ì„¸ì…˜ ì´ˆê¸°í™”
            if (i-1) % 10 == 0:
                print(f"   ğŸ”„ ChromeDriver ì„¸ì…˜ ì´ˆê¸°í™” ({i}ë²ˆì§¸ ë‰´ìŠ¤)")
                
                # ê¸°ì¡´ ë“œë¼ì´ë²„ ì •ë¦¬
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
                
                # ìƒˆ ë“œë¼ì´ë²„ ìƒì„±
                driver = setup_chrome_driver()
                if not driver:
                    print(f"   âŒ ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨ - í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break
                print("   âœ… ChromeDriver ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ê°œë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œë„ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)
            max_attempts = 5
            success = False
            
            for attempt in range(1, max_attempts + 1):
                try:
                    print(f"ğŸ“° [{i}] ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹œë„ {attempt}/{max_attempts}...")
                    
                    # ì„¸ì…˜ ìƒíƒœ í™•ì¸
                    try:
                        driver.current_url  # ì„¸ì…˜ í™•ì¸
                    except Exception as session_error:
                        print(f"   âš ï¸ ChromeDriver ì„¸ì…˜ ë¬¸ì œ ê°ì§€: {session_error}")
                        
                        # ì„¸ì…˜ ì¬ìƒì„±
                        try:
                            driver.quit()
                        except:
                            pass
                        
                        driver = setup_chrome_driver()
                        if not driver:
                            print(f"   âŒ ChromeDriver ì¬ìƒì„± ì‹¤íŒ¨")
                            raise Exception("ChromeDriver ì¬ìƒì„± ì‹¤íŒ¨")
                        print("   âœ… ChromeDriver ì¬ìƒì„± ì™„ë£Œ")
                    
                    # ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
                    news_info = crawl_news_detail(driver, news_item, i)
                    
                    if news_info and news_info['title'] != "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨":
                        news_data.append(news_info)
                        print(f"   âœ… ë‰´ìŠ¤ [{i}] ìˆ˜ì§‘ ì™„ë£Œ")
                        success = True
                        break
                    else:
                        raise Exception("ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"   âŒ ë‰´ìŠ¤ [{i}] ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
                    
                    if attempt < max_attempts:
                        wait_time = attempt * 2
                        print(f"   â° {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                        time.sleep(wait_time)
                    else:
                        print(f"   ğŸ’¥ ë‰´ìŠ¤ [{i}] ìµœì¢… ì‹¤íŒ¨ - ë‹¤ìŒ ë‰´ìŠ¤ë¡œ ì§„í–‰")
            
            if not success:
                print(f"   âš ï¸ ë‰´ìŠ¤ [{i}] ìˆ˜ì§‘ ì‹¤íŒ¨ - ë‹¤ìŒìœ¼ë¡œ ì´ë™")
            
            # ë‹¤ìŒ ë‰´ìŠ¤ë¡œ ì´ë™
            i += 1
            
            # ìš”ì²­ ê°„ê²© ì¡°ì •
            if i <= len(recent_news):
                time.sleep(1)
        
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(news_data)}ê°œ ìµœê·¼ ë‰´ìŠ¤ ìˆ˜ì§‘")
        
        # ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ ì ìš©
        time_filter_enabled = should_filter_by_time_recent()
        filtered_news_data = []
        filtered_count = 0
        
        if time_filter_enabled:
            print(f"\nğŸ” ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ ì ìš© ì¤‘...")
            for news in news_data:
                if is_news_time_valid_recent(news.get('pub_time'), time_filter_enabled):
                    filtered_news_data.append(news)
                else:
                    filtered_count += 1
            
            print(f"ğŸ•˜ ì‹œê°„ í•„í„°ë¡œ ì œì™¸ëœ ë‰´ìŠ¤: {filtered_count}ê°œ")
            print(f"âœ… ìµœì¢… ì²˜ë¦¬ëœ ë‰´ìŠ¤: {len(filtered_news_data)}ê°œ")
            final_news_data = filtered_news_data
        else:
            final_news_data = news_data
        
        return {
            'base_url': base_url,
            'status': 'ì„±ê³µ',
            'error': None,
            'news_count': len(final_news_data),
            'filtered_count': filtered_count,
            'news_list': final_news_data,
            'crawling_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return {
            'base_url': base_url,
            'status': 'ì‹¤íŒ¨',
            'error': str(e),
            'news_count': 0,
            'news_list': [],
            'crawling_time': datetime.now().isoformat()
        }
        
    finally:
        if driver:
            try:
                driver.quit()
                print("ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")
            except:
                pass
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if hasattr(driver, '_temp_dir'):
                try:
                    import shutil
                    shutil.rmtree(driver._temp_dir)
                    print(f"ğŸ§¹ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬: {driver._temp_dir}")
                except:
                    pass

def save_to_json(data, filename=None):
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    if filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        today = datetime.now().strftime('%Y%m%d')
        filename = f'medical_recent_news_{today}_{timestamp}.json'
    
    # crawler_result ë””ë ‰í† ë¦¬ì— ì €ì¥ - Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ ì‚¬ìš©
    result_dir = '/home/son/SKN12-FINAL-AIRFLOW/crawler_result'
    
    os.makedirs(result_dir, exist_ok=True)
    filepath = os.path.join(result_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
    return filepath

def main_with_retry(base_url=None, target_dates=None, max_retries=5):
    """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ëœ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ê¸°ë³¸ URL ì„¤ì •
    if base_url is None:
        base_url = "http://m.yakup.com/news/index.html?cat=11"
    
    # ê¸°ë³¸ ë‚ ì§œ ì„¤ì •
    if target_dates is None:
        # TEST_DATE í™˜ê²½ë³€ìˆ˜ í™•ì¸
        import os
        test_date_env = os.getenv('TEST_DATE')
        if test_date_env:
            try:
                # TEST_DATE íŒŒì‹± (í˜•ì‹: 'YYYY-MM-DD')
                test_date = datetime.strptime(test_date_env, '%Y-%m-%d').date()
                yesterday = test_date - timedelta(days=1)
                target_dates = {test_date, yesterday}
                print(f"ğŸ“… TEST_DATE ê¸°ë³¸ ë‚ ì§œ ì„¤ì •: {test_date.strftime('%Y.%m.%d')} (ì§€ì •ì¼), {yesterday.strftime('%Y.%m.%d')} (ì „ì¼)")
            except ValueError:
                print(f"âŒ ì˜ëª»ëœ TEST_DATE í˜•ì‹: {test_date_env}, í˜„ì¬ ë‚ ì§œ ì‚¬ìš©")
                today = datetime.now().date()
                yesterday = today - timedelta(days=1)
                target_dates = {today, yesterday}
                print(f"ğŸ“… ê¸°ë³¸ ë‚ ì§œ ì„¤ì •: {today.strftime('%Y.%m.%d')} (ì˜¤ëŠ˜), {yesterday.strftime('%Y.%m.%d')} (ì–´ì œ)")
        else:
            today = datetime.now().date()
            yesterday = today - timedelta(days=1)
            target_dates = {today, yesterday}
            print(f"ğŸ“… ê¸°ë³¸ ë‚ ì§œ ì„¤ì •: {today.strftime('%Y.%m.%d')} (ì˜¤ëŠ˜), {yesterday.strftime('%Y.%m.%d')} (ì–´ì œ)")
    
    result = None
    for attempt in range(1, max_retries + 1):
        try:
            print(f"\nğŸ”„ í¬ë¡¤ë§ ì‹œë„ {attempt}/{max_retries}")
            
            # Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ (ì¬ì‹œë„ ì „)
            try:
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
                time.sleep(2)  # í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ëŒ€ê¸°
                print("ğŸ§¹ ì´ì „ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            except:
                pass
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            result = crawl_recent_news(base_url, target_dates)
            
            # ì„±ê³µ ì—¬ë¶€ í™•ì¸
            if result['status'] == 'ì„±ê³µ' and result['news_count'] > 0:
                print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ! ({attempt}/{max_retries})")
                break
            else:
                raise Exception(f"í¬ë¡¤ë§ ê²°ê³¼ ë¶ˆëŸ‰: {result.get('error', 'ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨')}")
                
        except KeyboardInterrupt:
            print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        except Exception as e:
            print(f"âŒ ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
            
            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„ ì•ˆë‚´
            if attempt < max_retries:
                wait_time = attempt * 5  # ì¬ì‹œë„ ê°„ê²©ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€
                print(f"â° {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(wait_time)
            else:
                print(f"ğŸ’¥ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨. ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries}) ë„ë‹¬")
                # ë§ˆì§€ë§‰ ì‹¤íŒ¨ ì‹œì—ë„ ê²°ê³¼ ê°ì²´ ìƒì„±
                if result is None:
                    result = {
                        'base_url': base_url,
                        'status': 'ì‹¤íŒ¨',
                        'error': str(e),
                        'news_count': 0,
                        'news_list': [],
                        'crawling_time': datetime.now().isoformat()
                    }
        finally:
            # Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
            try:
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
            except:
                pass
    
    if result:
        # ê²°ê³¼ ì €ì¥
        filepath = save_to_json(result)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        print(f"ğŸŒ URL: {result['base_url']}")
        print(f"ğŸ“ˆ ìƒíƒœ: {result['status']}")
        print(f"ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {result['news_count']}ê°œ")
        
        if result['error']:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
        
        if result['news_list']:
            print("\nğŸ“‹ ìˆ˜ì§‘ëœ ìµœê·¼ ë‰´ìŠ¤ ëª©ë¡:")
            print("-" * 80)
            for news in result['news_list']:
                print(f"[{news['rank']:2d}] {news['title']}")
                print(f"     ğŸ“… ë‚ ì§œ: {news['date']}")
                if news['url']:
                    print(f"     ğŸ”— {news['url']}")
                if news['summary'] and news['summary'] not in ["", "ë³¸ë¬¸ ë‚´ìš© ë¶€ì¡±", "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"]:
                    print(f"     ğŸ“ ìš”ì•½: {news['summary']}")
                else:
                    # ìš”ì•½ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë‚´ìš©ì˜ ì¼ë¶€ í‘œì‹œ
                    if news['content'] and news['content'] != "ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨":
                        content_preview = news['content'][:100] + "..." if len(news['content']) > 100 else news['content']
                        print(f"     ğŸ“ ì›ë¬¸: {content_preview}")
                print()
        
        print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {filepath}")
        if result['status'] == 'ì„±ê³µ':
            print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        else:
            print("ğŸ’¥ í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨!")

def main(base_url=None, target_dates=None):
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
    main_with_retry(base_url, target_dates, max_retries=5)

if __name__ == "__main__":
    
    # ë‚ ì§œ ì„¤ì • (TEST_DATE í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
    base_url = "http://m.yakup.com/news/index.html?cat=11"
    
    # TEST_DATE í™˜ê²½ë³€ìˆ˜ í™•ì¸
    import os
    test_date_env = os.getenv('TEST_DATE')
    if test_date_env:
        try:
            # TEST_DATE íŒŒì‹± (í˜•ì‹: 'YYYY-MM-DD')
            target_date = datetime.strptime(test_date_env, '%Y-%m-%d').date()
            yesterday = target_date - timedelta(days=1)
            target_dates = {target_date, yesterday}
            print(f"ğŸ“… TEST_DATEë¡œ í¬ë¡¤ë§: {target_date.strftime('%Y.%m.%d')} (ì§€ì •ì¼), {yesterday.strftime('%Y.%m.%d')} (ì „ì¼)")
        except ValueError:
            print(f"âŒ ì˜ëª»ëœ TEST_DATE í˜•ì‹: {test_date_env}, í˜„ì¬ ë‚ ì§œ ì‚¬ìš©")
            target_date = datetime.now().date()  # ì˜¤ëŠ˜ ë‚ ì§œ
            yesterday = target_date - timedelta(days=1)  # ì–´ì œ ë‚ ì§œ ìë™ ê³„ì‚°
            target_dates = {target_date, yesterday}
    else:
        # ì˜¤ëŠ˜ ë‚ ì§œë¡œ í¬ë¡¤ë§ (ìë™ìœ¼ë¡œ ì–´ì œ ë‚ ì§œë„ í¬í•¨)
        target_date = datetime.now().date()  # ì˜¤ëŠ˜ ë‚ ì§œ
        yesterday = target_date - timedelta(days=1)  # ì–´ì œ ë‚ ì§œ ìë™ ê³„ì‚°
        target_dates = {target_date, yesterday}
    
    print(f"ğŸ¯ í¬ë¡¤ë§ URL: {base_url}")
    print(f"ğŸ¯ í¬ë¡¤ë§ ë‚ ì§œ: {target_date.strftime('%Y.%m.%d')} (ì˜¤ëŠ˜), {yesterday.strftime('%Y.%m.%d')} (ì–´ì œ)")
    
    main(base_url, target_dates)