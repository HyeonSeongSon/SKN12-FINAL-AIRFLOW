#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import time
from datetime import datetime, timedelta
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
import os
import tempfile
import uuid
import random
import subprocess

def setup_driver():
    """Chrome ë“œë¼ì´ë²„ ì„¤ì • (Docker í™˜ê²½ ìµœì í™”)"""
    
    # ê¸°ì¡´ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
    try:
        subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
        subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
        time.sleep(1)
        print("ğŸ§¹ ê¸°ì¡´ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass
    
    try:
        chrome_options = Options()
        
        # Docker í™˜ê²½ í•„ìˆ˜ ì˜µì…˜
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-software-rasterizer')
        chrome_options.add_argument('--window-size=1280,720')
        
        # SSL ë° ë³´ì•ˆ ê´€ë ¨ ì˜µì…˜
        chrome_options.add_argument('--ignore-ssl-errors-on-quic')
        chrome_options.add_argument('--ignore-certificate-errors')
        chrome_options.add_argument('--ignore-ssl-errors')
        chrome_options.add_argument('--ignore-certificate-errors-spki-list')
        chrome_options.add_argument('--disable-extensions-file-access-check')
        chrome_options.add_argument('--allow-running-insecure-content')
        
        # ì„¸ì…˜ ì¶©ëŒ ë°©ì§€ ë° ì•ˆì •ì„± ì˜µì…˜
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor,ChromeWhatsNewUI')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--disable-background-timer-throttling')
        chrome_options.add_argument('--disable-renderer-backgrounding')
        chrome_options.add_argument('--disable-backgrounding-occluded-windows')
        chrome_options.add_argument('--disable-client-side-phishing-detection')
        chrome_options.add_argument('--disable-component-extensions-with-background-pages')
        chrome_options.add_argument('--disable-ipc-flooding-protection')
        chrome_options.add_argument('--no-default-browser-check')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-background-networking')
        
        # ê³ ìœ  ì„¸ì…˜ ìƒì„±
        temp_dir = tempfile.mkdtemp(prefix=f'hira_chrome_{uuid.uuid4().hex[:8]}_')
        chrome_options.add_argument(f'--user-data-dir={temp_dir}')
        
        debug_port = random.randint(9500, 9999)
        chrome_options.add_argument(f'--remote-debugging-port={debug_port}')
        
        # ì„±ëŠ¥ ìµœì í™”
        chrome_options.add_argument('--memory-pressure-off')
        chrome_options.add_argument('--max_old_space_size=2048')
        chrome_options.add_argument('--aggressive-cache-discard')
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        print(f"ğŸ”§ Chrome ì„¸ì…˜ ë””ë ‰í† ë¦¬: {temp_dir}")
        print(f"ğŸ”§ ë””ë²„ê¹… í¬íŠ¸: {debug_port}")
        
        # ë“œë¼ì´ë²„ ìƒì„±
        driver = webdriver.Chrome(options=chrome_options)
        
        # ì„¸ì…˜ ì •ë³´ ì €ì¥
        driver._temp_dir = temp_dir
        driver._debug_port = debug_port
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        driver.implicitly_wait(10)
        driver.set_page_load_timeout(30)
        
        # ìë™í™” ê°ì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        print("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
        return driver
        
    except Exception as e:
        print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
        return None

def get_today_date():
    today = datetime.now()
    return today.strftime('%Y-%m-%d')

def get_yesterday_date():
    yesterday = datetime.now() - timedelta(days=1)
    return yesterday.strftime('%Y-%m-%d')

def crawl_page_data(driver, wait):
    """Crawl data from current page"""
    page_data = []
    
    tbody = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="cont"]/div[1]/div[3]/table/tbody')))
    rows = tbody.find_elements(By.TAG_NAME, 'tr')
    
    for i, row in enumerate(rows, 1):
        try:
            # Re-find elements each time to avoid stale reference
            tbody = driver.find_element(By.XPATH, '//*[@id="cont"]/div[1]/div[3]/table/tbody')
            current_row = tbody.find_element(By.XPATH, f'./tr[{i}]')
            
            td2_element = current_row.find_element(By.XPATH, './td[2]')
            td3_element = current_row.find_element(By.XPATH, './td[3]')
            td5_element = current_row.find_element(By.XPATH, './td[5]')
            
            ê³ ì‹œ = td2_element.text.strip()
            íƒ€ì´í‹€ = td3_element.text.strip()
            ë‚ ì§œ = td5_element.text.strip()
            
            url = None
            try:
                link_element = td3_element.find_element(By.TAG_NAME, 'a')
                if link_element:
                    # Store current window handle
                    main_window = driver.current_window_handle
                    
                    # Click the link (opens in new window)
                    link_element.click()
                    time.sleep(2)
                    
                    # Switch to new window
                    for window_handle in driver.window_handles:
                        if window_handle != main_window:
                            driver.switch_to.window(window_handle)
                            url = driver.current_url
                            driver.close()
                            break
                    
                    # Switch back to main window
                    driver.switch_to.window(main_window)
                    time.sleep(1)
            except:
                pass
            
            row_data = {
                'row_number': len(page_data) + 1,
                'ê³ ì‹œ': ê³ ì‹œ,
                'íƒ€ì´í‹€': íƒ€ì´í‹€,
                'ë‚ ì§œ': ë‚ ì§œ,
                'url': url,
                'crawled_date': datetime.now().isoformat()
            }
            
            page_data.append(row_data)
            
        except Exception as e:
            print(f"Error processing row {i}: {str(e)}")
            continue
    
    return page_data

def crawl_hira_data_with_retry(test_date=None, max_retries=3):
    """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ëœ HIRA ë°ì´í„° í¬ë¡¤ë§ í•¨ìˆ˜"""
    
    for attempt in range(1, max_retries + 1):
        print(f"\nğŸ”„ HIRA í¬ë¡¤ë§ ì‹œë„ {attempt}/{max_retries}")
        
        driver = None
        try:
            # Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ (ì¬ì‹œë„ ì „)
            try:
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
                time.sleep(2)
                print("ğŸ§¹ ì´ì „ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            except:
                pass
            
            # ìƒˆ ë“œë¼ì´ë²„ ìƒì„±
            driver = setup_driver()
            if not driver:
                raise Exception("Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            scraped_data = []
            url = "https://www.hira.or.kr/rc/insu/insuadtcrtr/InsuAdtCrtrList.do?pgmid=HIRAA030069000400&WT.gnb=%EB%B3%B4%ED%97%98%EC%9D%B8%EC%A0%95%EA%B8%B0%EC%A4%80"
            driver.get(url)
            
            wait = WebDriverWait(driver, 10)
            
            # Use yesterday for start date and today for end date
            if test_date:
                start_date = test_date
                end_date = test_date
            else:
                start_date = get_yesterday_date()  # ì–´ì œ ë‚ ì§œ
                end_date = get_today_date()       # ì˜¤ëŠ˜ ë‚ ì§œ
            
            start_date_input = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="startDt"]')))
            start_date_input.clear()
            start_date_input.send_keys(start_date)
            
            end_date_input = driver.find_element(By.XPATH, '//*[@id="endDt"]')
            end_date_input.clear()
            end_date_input.send_keys(end_date)
            
            search_button = driver.find_element(By.XPATH, '//*[@id="searchForm"]/div[2]/div[2]/a')
            search_button.click()
            
            time.sleep(3)
            
            # Pagination loop - continue until no more pages
            page_set_number = 1
            crawled_pages = set()  # Track crawled page numbers
            
            while True:
                print(f"Processing page set {page_set_number}")
                
                # Get all available pages in current set
                try:
                    paging_ul = driver.find_element(By.XPATH, '//*[@id="pagingPc"]/ul')
                    page_li_elements = paging_ul.find_elements(By.TAG_NAME, 'li')
                    print(f"Found {len(page_li_elements)} page buttons in current set")
                    
                    # Click through all li pages in current set
                    for i in range(len(page_li_elements)):
                        try:
                            # Re-find pagination elements to avoid stale references
                            paging_ul = driver.find_element(By.XPATH, '//*[@id="pagingPc"]/ul')
                            current_page_li_elements = paging_ul.find_elements(By.TAG_NAME, 'li')
                            
                            # Check if current index is still valid
                            if i >= len(current_page_li_elements):
                                print(f"Index {i} out of range, skipping...")
                                continue
                                
                            # Get page number from li element
                            page_link = current_page_li_elements[i].find_element(By.TAG_NAME, 'a')
                            page_text = page_link.text.strip()
                            
                            # Skip if already crawled
                            if page_text in crawled_pages:
                                print(f"Page {page_text} already crawled, skipping...")
                                continue
                            
                            print(f"Clicking page {page_text} (li index {i})...")
                            page_link.click()
                            time.sleep(2)
                            
                            # Crawl current page data
                            print(f"Crawling page {page_text}...")
                            page_data = crawl_page_data(driver, wait)
                            scraped_data.extend(page_data)
                            
                            # Mark as crawled
                            crawled_pages.add(page_text)
                            
                        except Exception as e:
                            print(f"Error processing li index {i}: {str(e)}")
                            continue
                    
                except Exception as e:
                    print(f"No pagination ul found or error: {str(e)}")
                    break
                
                # Look for next page set button (class="next")
                try:
                    next_button = driver.find_element(By.XPATH, '//*[@id="pagingPc"]/a[@class="next"]')
                    if next_button.is_enabled() and next_button.is_displayed():
                        print("Found next page set button, clicking...")
                        next_button.click()
                        time.sleep(3)
                        page_set_number += 1
                    else:
                        print("Next button exists but not clickable, finishing crawling")
                        break
                        
                except Exception as e:
                    print(f"No next page set button found: {str(e)}")
                    print("Finished crawling all available pages")
                    break
            
            # Update row numbers to be sequential across all pages
            for i, row in enumerate(scraped_data, 1):
                row['row_number'] = i
            
            # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨
            if scraped_data:
                print(f"âœ… HIRA í¬ë¡¤ë§ ì„±ê³µ! ({attempt}/{max_retries}) - {len(scraped_data)}ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘")
                return scraped_data
            else:
                print(f"âš ï¸ HIRA í¬ë¡¤ë§ ì™„ë£Œí–ˆì§€ë§Œ ë°ì´í„° ì—†ìŒ ({attempt}/{max_retries})")
                return []  # ë¹ˆ ê²°ê³¼ë„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        
        except Exception as e:
            print(f"âŒ HIRA í¬ë¡¤ë§ ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
            
            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„ ì•ˆë‚´
            if attempt < max_retries:
                wait_time = attempt * 5  # ì¬ì‹œë„ ê°„ê²©ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€
                print(f"â° {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(wait_time)
            else:
                print(f"ğŸ’¥ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨. ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries}) ë„ë‹¬")
        
        finally:
            # ê° ì‹œë„ë§ˆë‹¤ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
            if driver:
                try:
                    driver.quit()
                    print(f"ğŸ”š HIRA ë¸Œë¼ìš°ì € ì¢…ë£Œ (ì‹œë„ {attempt})")
                except:
                    pass
                
                # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                if hasattr(driver, '_temp_dir'):
                    try:
                        import shutil
                        shutil.rmtree(driver._temp_dir)
                        print(f"ğŸ§¹ HIRA ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬: {driver._temp_dir}")
                    except:
                        pass
            
            # Chrome í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì •ë¦¬
            try:
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
            except:
                pass
    
    # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
    print("ğŸ’¥ HIRA í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨!")
    return []

def crawl_hira_data(test_date=None):
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•¨ìˆ˜ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš©)"""
    return crawl_hira_data_with_retry(test_date, max_retries=3)

def save_to_json(data, filename=None):
    if filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'hira_data_{timestamp}.json'
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    filepath = os.path.join(current_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filepath

def main():
    print(f"HIRA ë°ì´í„° í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤ ({get_yesterday_date()} ~ {get_today_date()})...")
    print("ğŸ”„ ìµœëŒ€ 3íšŒ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í™œì„±í™”")
    
    data = crawl_hira_data()  # ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ëœ í•¨ìˆ˜ í˜¸ì¶œ
    
    if data:
        filepath = save_to_json(data)
        print(f"ğŸ‰ í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: {len(data)}ê°œ ë ˆì½”ë“œ")
        print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
    else:
        print("ğŸ’¥ í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨!")
        print("- ëª¨ë“  ì¬ì‹œë„ê°€ ì‹¤íŒ¨í–ˆê±°ë‚˜ í•´ë‹¹ ë‚ ì§œ ë²”ìœ„ì— HIRA ê³ ì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 2024-08-01 ~ 2025-08-01 ë‚ ì§œ ë²”ìœ„ë¡œ í…ŒìŠ¤íŠ¸ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)
def test_crawling_with_retry(max_retries=3):
    """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ëœ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ í•¨ìˆ˜"""
    
    for attempt in range(1, max_retries + 1):
        print(f"\nğŸ”„ HIRA í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œë„ {attempt}/{max_retries}")
        
        driver = None
        try:
            # Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ (ì¬ì‹œë„ ì „)
            try:
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
                time.sleep(2)
                print("ğŸ§¹ ì´ì „ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            except:
                pass
            
            # ìƒˆ ë“œë¼ì´ë²„ ìƒì„±
            driver = setup_driver()
            if not driver:
                raise Exception("Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
            
            url = "https://www.hira.or.kr/rc/insu/insuadtcrtr/InsuAdtCrtrList.do?pgmid=HIRAA030069000400&WT.gnb=%EB%B3%B4%ED%97%98%EC%9D%B8%EC%A0%95%EA%B8%B0%EC%A4%80"
            driver.get(url)
            
            wait = WebDriverWait(driver, 10)
            
            # Set specific date range for test
            start_date_input = wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id="startDt"]')))
            start_date_input.clear()
            start_date_input.send_keys("2024-08-01")
            
            end_date_input = driver.find_element(By.XPATH, '//*[@id="endDt"]')
            end_date_input.clear()
            end_date_input.send_keys("2025-08-01")
            
            search_button = driver.find_element(By.XPATH, '//*[@id="searchForm"]/div[2]/div[2]/a')
            search_button.click()
            
            time.sleep(3)
            
            # Pagination loop - continue until no more pages
            page_set_number = 1
            crawled_pages = set()  # Track crawled page numbers
            scraped_data = []
            
            while True:
                print(f"Processing page set {page_set_number}")
                
                # Get all available pages in current set
                try:
                    paging_ul = driver.find_element(By.XPATH, '//*[@id="pagingPc"]/ul')
                    page_li_elements = paging_ul.find_elements(By.TAG_NAME, 'li')
                    print(f"Found {len(page_li_elements)} page buttons in current set")
                    
                    # Click through all li pages in current set
                    for i in range(len(page_li_elements)):
                        try:
                            # Re-find pagination elements to avoid stale references
                            paging_ul = driver.find_element(By.XPATH, '//*[@id="pagingPc"]/ul')
                            current_page_li_elements = paging_ul.find_elements(By.TAG_NAME, 'li')
                            
                            # Check if current index is still valid
                            if i >= len(current_page_li_elements):
                                print(f"Index {i} out of range, skipping...")
                                continue
                                
                            # Get page number from li element
                            page_link = current_page_li_elements[i].find_element(By.TAG_NAME, 'a')
                            page_text = page_link.text.strip()
                            
                            # Skip if already crawled
                            if page_text in crawled_pages:
                                print(f"Page {page_text} already crawled, skipping...")
                                continue
                            
                            print(f"Clicking page {page_text} (li index {i})...")
                            page_link.click()
                            time.sleep(2)
                            
                            # Crawl current page data
                            print(f"Crawling page {page_text}...")
                            page_data = crawl_page_data(driver, wait)
                            scraped_data.extend(page_data)
                            
                            # Mark as crawled
                            crawled_pages.add(page_text)
                            
                        except Exception as e:
                            print(f"Error processing li index {i}: {str(e)}")
                            continue
                    
                except Exception as e:
                    print(f"No pagination ul found or error: {str(e)}")
                    break
                
                # Look for next page set button (class="next")
                try:
                    next_button = driver.find_element(By.XPATH, '//*[@id="pagingPc"]/a[@class="next"]')
                    if next_button.is_enabled() and next_button.is_displayed():
                        print("Found next page set button, clicking...")
                        next_button.click()
                        time.sleep(3)
                        page_set_number += 1
                    else:
                        print("Next button exists but not clickable, finishing crawling")
                        break
                        
                except Exception as e:
                    print(f"No next page set button found: {str(e)}")
                    print("Finished crawling all available pages")
                    break
            
            # Update row numbers to be sequential across all pages
            for i, row in enumerate(scraped_data, 1):
                row['row_number'] = i
            
            # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨
            if scraped_data:
                print(f"âœ… HIRA í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì„±ê³µ! ({attempt}/{max_retries}) - {len(scraped_data)}ê°œ ë ˆì½”ë“œ ìˆ˜ì§‘")
                return scraped_data
            else:
                print(f"âš ï¸ HIRA í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì™„ë£Œí–ˆì§€ë§Œ ë°ì´í„° ì—†ìŒ ({attempt}/{max_retries})")
                return []  # ë¹ˆ ê²°ê³¼ë„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼
        
        except Exception as e:
            print(f"âŒ HIRA í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
            
            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„ ì•ˆë‚´
            if attempt < max_retries:
                wait_time = attempt * 5  # ì¬ì‹œë„ ê°„ê²©ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€
                print(f"â° {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(wait_time)
            else:
                print(f"ğŸ’¥ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨. ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries}) ë„ë‹¬")
        
        finally:
            # ê° ì‹œë„ë§ˆë‹¤ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
            if driver:
                try:
                    driver.quit()
                    print(f"ğŸ”š HIRA í…ŒìŠ¤íŠ¸ ë¸Œë¼ìš°ì € ì¢…ë£Œ (ì‹œë„ {attempt})")
                except:
                    pass
                
                # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
                if hasattr(driver, '_temp_dir'):
                    try:
                        import shutil
                        shutil.rmtree(driver._temp_dir)
                        print(f"ğŸ§¹ HIRA í…ŒìŠ¤íŠ¸ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬: {driver._temp_dir}")
                    except:
                        pass
            
            # Chrome í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì •ë¦¬
            try:
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
            except:
                pass
    
    # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
    print("ğŸ’¥ HIRA í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨!")
    return []

def test_crawling():
    """í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ê¸°ì¡´ í•¨ìˆ˜ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ì‚¬ìš©)"""
    print("ğŸ”„ ìµœëŒ€ 3íšŒ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í™œì„±í™”")
    data = test_crawling_with_retry(max_retries=3)
    
    if data:
        filepath = save_to_json(data, "hira_data_test_range.json")
        print(f"ğŸ‰ í…ŒìŠ¤íŠ¸: í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤ - {len(data)}ê°œ ë ˆì½”ë“œ")
        print(f"ğŸ’¾ í…ŒìŠ¤íŠ¸: ë°ì´í„° ì €ì¥ ì™„ë£Œ - {filepath}")
    else:
        print("ğŸ’¥ í…ŒìŠ¤íŠ¸: í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨!")
        print("- ëª¨ë“  ì¬ì‹œë„ê°€ ì‹¤íŒ¨í–ˆê±°ë‚˜ í•´ë‹¹ ë‚ ì§œ ë²”ìœ„ì— HIRA ê³ ì‹œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")

if __name__ == "__main__":
    main()