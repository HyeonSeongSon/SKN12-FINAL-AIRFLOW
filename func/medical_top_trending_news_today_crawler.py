#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ì•½ì—…ë‹·ì»´ ì˜ë£Œë‰´ìŠ¤ í¬ë¡¤ëŸ¬
ì •í™•í•œ XPathë¥¼ ì‚¬ìš©í•˜ì—¬ dl ìš”ì†Œì—ì„œ ë‰´ìŠ¤ ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ê³  ìƒì„¸ ì •ë³´ë¥¼ í¬ë¡¤ë§
"""

import json
import time
from datetime import datetime, timedelta
import os
import tempfile
import uuid
import random
import subprocess
import re
from dateutil import parser
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import openai
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

def setup_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ì„¤ì • (Docker í™˜ê²½ ìµœì í™”)"""
    
    # ê¸°ì¡´ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
    try:
        subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
        subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
        time.sleep(1)
        print("ğŸ§¹ ê¸°ì¡´ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass
    
    try:
        chrome_options = Options()
        
        # Docker í™˜ê²½ í•„ìˆ˜ ì˜µì…˜
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-software-rasterizer')
        chrome_options.add_argument('--window-size=1280,720')
        
        # ì„¸ì…˜ ì¶©ëŒ ë°©ì§€ ë° ì•ˆì •ì„± ì˜µì…˜
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor,ChromeWhatsNewUI')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--disable-background-timer-throttling')
        chrome_options.add_argument('--disable-renderer-backgrounding')
        chrome_options.add_argument('--disable-backgrounding-occluded-windows')
        chrome_options.add_argument('--disable-client-side-phishing-detection')
        chrome_options.add_argument('--disable-component-extensions-with-background-pages')
        chrome_options.add_argument('--disable-ipc-flooding-protection')
        chrome_options.add_argument('--no-default-browser-check')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-background-networking')
        
        # ê³ ìœ  ì„¸ì…˜ ìƒì„±
        temp_dir = tempfile.mkdtemp(prefix=f'yakup_chrome_{uuid.uuid4().hex[:8]}_')
        chrome_options.add_argument(f'--user-data-dir={temp_dir}')
        
        debug_port = random.randint(9500, 9999)
        chrome_options.add_argument(f'--remote-debugging-port={debug_port}')
        
        # ì„±ëŠ¥ ìµœì í™” ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ (DAG í™˜ê²½ìš©)
        chrome_options.add_argument('--memory-pressure-off')
        chrome_options.add_argument('--max_old_space_size=2048')  
        chrome_options.add_argument('--aggressive-cache-discard')
        chrome_options.add_argument('--max-connections-per-host=6')
        chrome_options.add_argument('--max-connections-per-proxy=2')
        chrome_options.add_argument('--disable-features=TranslateUI')
        chrome_options.add_argument('--disable-features=MediaRouter')
        chrome_options.add_argument('--disable-hang-monitor')
        chrome_options.add_argument('--disable-prompt-on-repost')
        
        # User Agent ì„¤ì • (ëª¨ë°”ì¼)
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1')
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        print(f"ğŸ”§ Chrome ì„¸ì…˜ ë””ë ‰í† ë¦¬: {temp_dir}")
        print(f"ğŸ”§ ë””ë²„ê¹… í¬íŠ¸: {debug_port}")
        
        # Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” - Docker í™˜ê²½ì—ì„œëŠ” ì‹œìŠ¤í…œ ChromeDriver ì‚¬ìš©
        driver = None
        try:
            # Docker í™˜ê²½ì—ì„œëŠ” ì‹œìŠ¤í…œ ChromeDriver ì§ì ‘ ì‚¬ìš© (ì•„í‚¤í…ì²˜ í˜¸í™˜ì„± ë¬¸ì œ ë°©ì§€)
            driver = webdriver.Chrome(options=chrome_options)
            
        except Exception as e:
            print(f"âŒ Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
        
        if driver:
            # íƒ€ì„ì•„ì›ƒ ë° ê¸°ë³¸ ì„¤ì •
            driver.implicitly_wait(10)
            driver.set_page_load_timeout(30)
            
            # ì„¸ì…˜ ì •ë³´ ì €ì¥ (ì •ë¦¬ìš©)
            driver._temp_dir = temp_dir
            driver._debug_port = debug_port
            
            # ìë™í™” ê°ì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì™„ë£¼")
            return driver
        
    except Exception as e:
        print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
        return None

def collect_news_urls(driver):
    """ë‰´ìŠ¤ URL ìˆ˜ì§‘ (ì§€ì •ëœ XPath íŒ¨í„´ ì‚¬ìš©)"""
    print("ğŸ” ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì‹œì‘...")
    
    news_urls = []
    
    # ì»¨í…Œì´ë„ˆ 1: dl[n]/dt/a íŒ¨í„´
    container1_xpath = "//*[@id='main_con']/div[1]/div/div[2]/div[2]/div[1]"
    print(f"ğŸ“¦ ì»¨í…Œì´ë„ˆ 1 ê²€ìƒ‰ (dl/dt/a íŒ¨í„´): {container1_xpath}")
    
    try:
        container1 = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.XPATH, container1_xpath))
        )
        print(f"âœ… ì»¨í…Œì´ë„ˆ 1 ë°œê²¬")
        
        # dl ìš”ì†Œë“¤ ì°¾ê¸°
        dl_elements = container1.find_elements(By.TAG_NAME, "dl")
        print(f"   ğŸ“„ dl ìš”ì†Œ {len(dl_elements)}ê°œ ë°œê²¬")
        
        for dl_idx, dl_element in enumerate(dl_elements, 1):
            try:
                # dt/a ë§í¬ ì°¾ê¸°
                link_selectors = [
                    ".//dt/a",      # ì¼ë°˜ì ì¸ íŒ¨í„´
                    ".//dt/a[1]"    # ì²« ë²ˆì§¸ ë§í¬
                ]
                
                link_found = False
                for selector in link_selectors:
                    try:
                        link_element = dl_element.find_element(By.XPATH, selector)
                        url = link_element.get_attribute('href')
                        title_preview = link_element.text.strip()[:40]
                        
                        if url and url not in news_urls:
                            news_urls.append(url)
                            print(f"     [{len(news_urls)}] ğŸ“° {title_preview}...")
                            print(f"         ğŸ”— {url}")
                            link_found = True
                            break
                    except:
                        continue
                
                if not link_found:
                    print(f"     âŒ dl[{dl_idx}]ì—ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
            except Exception as e:
                print(f"     âŒ dl[{dl_idx}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
                
    except TimeoutException:
        print(f"âŒ ì»¨í…Œì´ë„ˆ 1ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    except Exception as e:
        print(f"âŒ ì»¨í…Œì´ë„ˆ 1 ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    # ì»¨í…Œì´ë„ˆ 2: p[n]/a íŒ¨í„´
    container2_xpath = "//*[@id='main_con']/div[1]/div/div[2]/div[2]/div[2]"
    print(f"ğŸ“¦ ì»¨í…Œì´ë„ˆ 2 ê²€ìƒ‰ (p/a íŒ¨í„´): {container2_xpath}")
    
    try:
        container2 = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.XPATH, container2_xpath))
        )
        print(f"âœ… ì»¨í…Œì´ë„ˆ 2 ë°œê²¬")
        
        # p ìš”ì†Œë“¤ ì°¾ê¸°
        p_elements = container2.find_elements(By.TAG_NAME, "p")
        print(f"   ğŸ“„ p ìš”ì†Œ {len(p_elements)}ê°œ ë°œê²¬")
        
        for p_idx, p_element in enumerate(p_elements, 1):
            try:
                # p/a ë§í¬ ì°¾ê¸°
                link_selectors = [
                    ".//a",      # ì¼ë°˜ì ì¸ íŒ¨í„´
                    ".//a[1]"    # ì²« ë²ˆì§¸ ë§í¬
                ]
                
                link_found = False
                for selector in link_selectors:
                    try:
                        link_element = p_element.find_element(By.XPATH, selector)
                        url = link_element.get_attribute('href')
                        title_preview = link_element.text.strip()[:40]
                        
                        if url and url not in news_urls:
                            news_urls.append(url)
                            print(f"     [{len(news_urls)}] ğŸ“° {title_preview}...")
                            print(f"         ğŸ”— {url}")
                            link_found = True
                            break
                    except:
                        continue
                
                if not link_found:
                    print(f"     âŒ p[{p_idx}]ì—ì„œ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
            except Exception as e:
                print(f"     âŒ p[{p_idx}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
                
    except TimeoutException:
        print(f"âŒ ì»¨í…Œì´ë„ˆ 2ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    except Exception as e:
        print(f"âŒ ì»¨í…Œì´ë„ˆ 2 ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    print(f"ğŸ¯ ì´ {len(news_urls)}ê°œì˜ ë‰´ìŠ¤ URL ìˆ˜ì§‘ ì™„ë£Œ")
    return news_urls

def extract_article_date_yakup(driver, soup=None):
    """ì•½ì—…ë‹·ì»´ ê¸°ì‚¬ì—ì„œ ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ (ì§ì ‘ XPath ì‚¬ìš©)"""
    try:
        # ì‚¬ìš©ìê°€ ì œê³µí•œ ì •í™•í•œ XPath ì‚¬ìš©
        date_xpath = "//*[@id='main_con']/div[1]/div/div[1]/div[1]/div[2]/div[2]"
        
        try:
            date_element = WebDriverWait(driver, 3).until(
                EC.presence_of_element_located((By.XPATH, date_xpath))
            )
            date_text = date_element.text.strip()
            
            if date_text:
                print(f"    ğŸ” ì—…ë¡œë“œ ë‚ ì§œ ë°œê²¬: {date_text}")
                return parse_and_format_date_yakup(date_text)
            else:
                print("    âš ï¸ ë‚ ì§œ ìš”ì†ŒëŠ” ìˆì§€ë§Œ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
                
        except TimeoutException:
            print(f"    âš ï¸ ì§€ì •ëœ XPathì—ì„œ ë‚ ì§œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {date_xpath}")
        except Exception as e:
            print(f"    âŒ ë‚ ì§œ ìš”ì†Œ ì ‘ê·¼ ì˜¤ë¥˜: {e}")
        
        # ë°±ì—…: ë‹¤ë¥¸ ê°€ëŠ¥í•œ ìœ„ì¹˜ë“¤ ë¹ ë¥´ê²Œ í™•ì¸
        backup_selectors = [
            "//*[@id='main_con']/div[1]/div/div[1]/div[2]/div[1]/span",
            "//*[@id='main_con']/div[1]/div/div[1]/div[2]/div[1]",
            "//*[@id='main_con']/div[1]/div/div[1]/div[2]/span"
        ]
        
        for selector in backup_selectors:
            try:
                elem = driver.find_element(By.XPATH, selector)
                text = elem.text.strip()
                if text and (re.search(r'\d{4}', text) or 'ì‹œê°„ ì „' in text or 'ë¶„ ì „' in text):
                    print(f"    ğŸ” ë°±ì—…ìœ¼ë¡œ ë‚ ì§œ ë°œê²¬: {selector} -> {text}")
                    return parse_and_format_date_yakup(text)
            except:
                continue
        
        print("    âš ï¸ ëª¨ë“  ì‹œë„ì—ì„œ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        return None
        
    except Exception as e:
        print(f"    âŒ ë‚ ì§œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

def parse_and_format_date_yakup(date_text):
    """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±í•˜ì—¬ YYYY.MM.DD hh:mm í˜•íƒœë¡œ ë³€í™˜"""
    try:
        current_time = datetime.now()
        
        print(f"    ğŸ” ë‚ ì§œ íŒŒì‹± ì‹œë„: '{date_text}'")
        
        # "ì…ë ¥"ê³¼ "ìˆ˜ì •" ë‚ ì§œê°€ í•¨ê»˜ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if 'ì…ë ¥' in date_text:
            # "ì…ë ¥ 2025-08-12 09:34 ìˆ˜ì • 2025.08.12 09:34" ì—ì„œ ì…ë ¥ ë‚ ì§œë§Œ ì¶”ì¶œ
            input_match = re.search(r'ì…ë ¥[^\d]*(\d{4}[-.\s]\d{1,2}[-.\s]\d{1,2}[^\d]*\d{1,2}:\d{2})', date_text)
            if input_match:
                input_date = input_match.group(1).strip()
                print(f"    âœ… ì…ë ¥ ë‚ ì§œ ì¶”ì¶œ: '{input_date}'")
                # í•˜ì´í”ˆì„ ì ìœ¼ë¡œ ë³€í™˜í•˜ê³  ê³µë°± ì œê±°
                input_date = re.sub(r'[-\s]', '.', input_date)
                input_date = re.sub(r'\.+', '.', input_date)  # ì—°ì†ëœ ì  ì œê±°
                # ì¬ê·€ í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ì²˜ë¦¬
                try:
                    dt = parser.parse(input_date, fuzzy=True)
                    return dt.strftime("%Y.%m.%d %H:%M")
                except:
                    # ìˆ˜ë™ íŒŒì‹±
                    parts = input_date.split()
                    if len(parts) >= 2:
                        date_part = parts[0].replace('.', '-')
                        time_part = parts[1]
                        try:
                            dt = datetime.strptime(f"{date_part} {time_part}", "%Y-%m-%d %H:%M")
                            return dt.strftime("%Y.%m.%d %H:%M")
                        except:
                            pass
                    return current_time.strftime("%Y.%m.%d %H:%M")
            
            # ì‹œê°„ì´ ì—†ëŠ” ê²½ìš°: "ì…ë ¥ 2025-08-12"
            input_match_no_time = re.search(r'ì…ë ¥[^\d]*(\d{4}[-.\s]\d{1,2}[-.\s]\d{1,2})', date_text)
            if input_match_no_time:
                input_date = input_match_no_time.group(1).strip() + " 00:00"
                print(f"    âœ… ì…ë ¥ ë‚ ì§œ ì¶”ì¶œ (ì‹œê°„ ì—†ìŒ): '{input_date}'")
                input_date = re.sub(r'[-\s]', '.', input_date)
                # ì¬ê·€ í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ì²˜ë¦¬
                try:
                    dt = parser.parse(input_date, fuzzy=True)
                    return dt.strftime("%Y.%m.%d %H:%M")
                except:
                    return current_time.strftime("%Y.%m.%d %H:%M")
        
        # "ì‘ì„±"ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬ (SBSì™€ ìœ ì‚¬í•œ íŒ¨í„´)
        elif 'ì‘ì„±' in date_text:
            created_match = re.search(r'ì‘ì„±[^\d]*(\d{4}[-.\s]\d{1,2}[-.\s]\d{1,2}[^\d]*\d{1,2}:\d{2})', date_text)
            if created_match:
                created_date = created_match.group(1).strip()
                print(f"    âœ… ì‘ì„± ë‚ ì§œ ì¶”ì¶œ: '{created_date}'")
                created_date = re.sub(r'[-\s]', '.', created_date)
                created_date = re.sub(r'\.+', '.', created_date)
                # ì¬ê·€ í˜¸ì¶œ ë°©ì§€ë¥¼ ìœ„í•´ ì§ì ‘ ì²˜ë¦¬
                try:
                    dt = parser.parse(created_date, fuzzy=True)
                    return dt.strftime("%Y.%m.%d %H:%M")
                except:
                    return current_time.strftime("%Y.%m.%d %H:%M")
        
        # ìƒëŒ€ì  ì‹œê°„ í‘œí˜„ ì²˜ë¦¬
        elif 'ì‹œê°„ ì „' in date_text:
            hours_ago = int(re.search(r'(\d+)ì‹œê°„', date_text).group(1))
            target_time = current_time - timedelta(hours=hours_ago)
            return target_time.strftime("%Y.%m.%d %H:%M")
        
        elif 'ë¶„ ì „' in date_text:
            minutes_ago = int(re.search(r'(\d+)ë¶„', date_text).group(1))
            target_time = current_time - timedelta(minutes=minutes_ago)
            return target_time.strftime("%Y.%m.%d %H:%M")
        
        elif 'ì¼ ì „' in date_text:
            days_ago = int(re.search(r'(\d+)ì¼', date_text).group(1))
            target_time = current_time - timedelta(days=days_ago)
            return target_time.strftime("%Y.%m.%d %H:%M")
        
        # ì•½ì—…ë‹·ì»´ íŠ¹ìˆ˜ í˜•ì‹: "2024. 1. 15. 14:30"
        elif re.match(r'\d{4}\. \d{1,2}\. \d{1,2}\. \d{1,2}:\d{2}', date_text):
            # "2024. 1. 15. 14:30" -> "2024.01.15 14:30"
            parts = date_text.split('. ')
            if len(parts) >= 4:
                year = parts[0]
                month = parts[1].zfill(2)
                day = parts[2].zfill(2)
                time_part = parts[3] if ':' in parts[3] else "00:00"
                return f"{year}.{month}.{day} {time_part}"
        
        # í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        elif 'ë…„' in date_text and 'ì›”' in date_text and 'ì¼' in date_text:
            date_match = re.search(r'(\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼\s*(\d{1,2}):(\d{2})', date_text)
            if date_match:
                year, month, day, hour, minute = date_match.groups()
                return f"{year}.{month.zfill(2)}.{day.zfill(2)} {hour.zfill(2)}:{minute}"
        
        # í•˜ì´í”ˆ í˜•ì‹ ë‚ ì§œ ì²˜ë¦¬: "2025-08-12 09:34" ë“±
        elif re.search(r'\d{4}-\d{1,2}-\d{1,2}', date_text):
            try:
                # ì‹œê°„ì´ ìˆëŠ” ê²½ìš°: "2025-08-12 09:34"
                datetime_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})\s+(\d{1,2}):(\d{2})', date_text)
                if datetime_match:
                    year, month, day, hour, minute = datetime_match.groups()
                    result = f"{year}.{month.zfill(2)}.{day.zfill(2)} {hour.zfill(2)}:{minute}"
                    print(f"    âœ… í•˜ì´í”ˆ í˜•ì‹ ë‚ ì§œ íŒŒì‹±: '{result}'")
                    return result
                
                # ë‚ ì§œë§Œ ìˆëŠ” ê²½ìš°: "2025-08-12"
                date_match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', date_text)
                if date_match:
                    year, month, day = date_match.groups()
                    result = f"{year}.{month.zfill(2)}.{day.zfill(2)} 00:00"
                    print(f"    âœ… í•˜ì´í”ˆ í˜•ì‹ ë‚ ì§œ íŒŒì‹± (ì‹œê°„ ì—†ìŒ): '{result}'")
                    return result
                    
            except Exception as e:
                print(f"    âŒ í•˜ì´í”ˆ í˜•ì‹ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        # ì¼ë°˜ì ì¸ ë‚ ì§œ í˜•ì‹ë“¤ ì‹œë„
        date_formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d %H:%M',
            '%Y.%m.%d %H:%M',
            '%Y/%m/%d %H:%M',
            '%Y-%m-%d',
            '%Y.%m.%d',
            '%Y/%m/%d'
        ]
        
        for fmt in date_formats:
            try:
                dt = datetime.strptime(date_text.strip(), fmt)
                return dt.strftime("%Y.%m.%d %H:%M")
            except ValueError:
                continue
        
        # dateutil.parser ì‚¬ìš© (ë§ˆì§€ë§‰ ì‹œë„)
        try:
            dt = parser.parse(date_text, fuzzy=True)
            return dt.strftime("%Y.%m.%d %H:%M")
        except:
            pass
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, í˜„ì¬ ì‹œê°„ ì‚¬ìš©: '{date_text}'")
        return current_time.strftime("%Y.%m.%d %H:%M")
        
    except Exception as e:
        print(f"    âŒ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return datetime.now().strftime("%Y.%m.%d %H:%M")

def crawl_news_detail(driver, news_url, rank):
    """ê°œë³„ ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§"""
    print(f"ğŸ“° [{rank}] ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì¤‘...")
    print(f"   ğŸŒ URL: {news_url}")
    
    try:
        # í•´ë‹¹ URLë¡œ ì´ë™
        driver.get(news_url)
        time.sleep(2)
        
        news_info = {
            'rank': rank,
            'title': '',
            'content': '',
            'summary': '',
            'url': news_url,
            'date': datetime.now().strftime('%Y%m%d'),
            'pub_time': None,  # ì—…ë¡œë“œ ë‚ ì§œ/ì‹œê°„
            'source': 'yakup.com',
            'type': 'medical news'
        }
        
        # ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ
        print("   ğŸ“… ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ ì¤‘...")
        pub_time = extract_article_date_yakup(driver)
        if pub_time:
            news_info['pub_time'] = pub_time
            print(f"   âœ… ì—…ë¡œë“œ ë‚ ì§œ: {pub_time}")
        else:
            news_info['pub_time'] = datetime.now().strftime("%Y.%m.%d %H:%M")
            print("   âš ï¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨, í˜„ì¬ ì‹œê°„ ì‚¬ìš©")
        
        # ì œëª© ì¶”ì¶œ (//*[@id="main_con"]/div[1]/div/div[1]/div[1])
        try:
            title_xpath = "//*[@id='main_con']/div[1]/div/div[1]/div[1]"
            title_element = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.XPATH, title_xpath))
            )
            news_info['title'] = title_element.text.strip()
            print(f"   âœ… ì œëª©: {news_info['title']}")
        except Exception as e:
            print(f"   âŒ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            news_info['title'] = f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨"
        
        # ë‚´ìš© ì¶”ì¶œ (//*[@id="main_con"]/div[1]/div/div[1]/div[2]/div[2]/span)
        try:
            content_xpath = "//*[@id='main_con']/div[1]/div/div[1]/div[2]/div[2]/span"
            content_elements = driver.find_elements(By.XPATH, content_xpath)
            
            if content_elements:
                content_texts = []
                for elem in content_elements:
                    text = elem.text.strip()
                    if text:
                        content_texts.append(text)
                
                news_info['content'] = ' '.join(content_texts)
                print(f"   âœ… ë‚´ìš©: {len(news_info['content'])}ì ì¶”ì¶œ")
            else:
                print(f"   âŒ ë‚´ìš© ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                news_info['content'] = "ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"
                
        except Exception as e:
            print(f"   âŒ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            news_info['content'] = "ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"
        
        return news_info
        
    except Exception as e:
        print(f"   âŒ ë‰´ìŠ¤ [{rank}] ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return None

def summarize_with_openai(title, content):
    """OpenAI APIë¡œ ê¸°ì‚¬ ìš”ì•½"""
    try:
        # OpenAI API í‚¤ í™•ì¸
        if not os.getenv('OPENAI_API_KEY'):
            print("    âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
            return None
        
        if not content or len(content.strip()) < 50 or "ì¶”ì¶œ ì‹¤íŒ¨" in content:
            return "ë³¸ë¬¸ ë‚´ìš©ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        if len(content) > 3000:
            content = content[:3000] + "..."
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = openai.OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = f"""ë‹¤ìŒ ì˜ë£Œ/ì œì•½ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.
        
ì œëª©: {title}

ë‚´ìš©: {content}

ìš”ì•½:"""
        
        # OpenAI API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=200
        )
        
        summary = response.choices[0].message.content.strip()
        return summary
        
    except Exception as e:
        print(f"    âŒ OpenAI ìš”ì•½ ì˜¤ë¥˜: {e}")
        return None

def should_filter_by_time_yakup():
    """í˜„ì¬ ì‹œê°„ì— ë”°ë¥¸ ë‰´ìŠ¤ í•„í„°ë§ ì—¬ë¶€ ê²°ì •"""
    current_hour = datetime.now().hour
    
    if current_hour >= 13:
        print(f"â° í˜„ì¬ ì‹œê°: {current_hour}ì‹œ - 09ì‹œ ì´í›„ ë‰´ìŠ¤ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        return True
    else:
        print(f"â° í˜„ì¬ ì‹œê°: {current_hour}ì‹œ - ëª¨ë“  ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
        return False

def is_news_time_valid_yakup(news_pub_time, filter_enabled):
    """ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ì´ í•„í„°ë§ ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸"""
    if not filter_enabled or not news_pub_time:
        return True
    
    try:
        # YYYY.MM.DD hh:mm í˜•ì‹ íŒŒì‹±
        news_datetime = datetime.strptime(news_pub_time, "%Y.%m.%d %H:%M")
        today = datetime.now().date()
        
        # ì˜¤ëŠ˜ ë‚ ì§œì¸ ê²½ìš°ë§Œ ì‹œê°„ í•„í„° ì ìš©
        if news_datetime.date() == today:
            if news_datetime.hour >= 9:
                print(f"    âœ… ì‹œê°„ í•„í„° í†µê³¼: {news_pub_time} (09ì‹œ ì´í›„)")
                return True
            else:
                print(f"    âŒ ì‹œê°„ í•„í„° ì œì™¸: {news_pub_time} (09ì‹œ ì´ì „)")
                return False
        else:
            # ì˜¤ëŠ˜ì´ ì•„ë‹Œ ë‚ ì§œëŠ” ëª¨ë‘ í¬í•¨
            print(f"    âœ… ë‚ ì§œ í•„í„° í†µê³¼: {news_pub_time} (ì˜¤ëŠ˜ì´ ì•„ë‹Œ ë‚ ì§œ)")
            return True
            
    except Exception as e:
        print(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜, ë‰´ìŠ¤ í¬í•¨: {news_pub_time} - {e}")
        return True

def add_summaries(news_list, filter_enabled=False):
    """ë‰´ìŠ¤ ëª©ë¡ì— AI ìš”ì•½ ì¶”ê°€ (ì‹œê°„ í•„í„°ë§ í¬í•¨)"""
    print(f"\nğŸ¤– AI ìš”ì•½ ìƒì„± ì¤‘... ({len(news_list)}ê°œ)")
    
    processed_news = []
    filtered_count = 0
    
    for i, news in enumerate(news_list, 1):
        print(f"\n[{i}/{len(news_list)}] {news['title'][:50]}...")
        
        # ì‹œê°„ í•„í„°ë§ ê²€ì‚¬
        if not is_news_time_valid_yakup(news.get('pub_time'), filter_enabled):
            filtered_count += 1
            print(f"    ğŸš« ì‹œê°„ ì¡°ê±´ìœ¼ë¡œ ì¸í•´ ì œì™¸ë¨")
            continue
        
        if news['content'] and "ì¶”ì¶œ ì‹¤íŒ¨" not in news['content']:
            print("    ğŸ¤– AI ìš”ì•½ ìƒì„± ì¤‘...")
            summary = summarize_with_openai(news['title'], news['content'])
            
            if summary:
                news['summary'] = summary
                print(f"    ğŸ“ ìš”ì•½: {summary}")
            else:
                news['summary'] = "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"
                print("    âŒ AI ìš”ì•½ ìƒì„± ì‹¤íŒ¨")
        else:
            news['summary'] = "ë³¸ë¬¸ ë‚´ìš© ë¶€ì¡±"
            print("    âŒ ë³¸ë¬¸ ë‚´ìš©ì´ ë¶€ì¡±í•˜ì—¬ ìš”ì•½ ë¶ˆê°€")
        
        processed_news.append(news)
        
        # API ìš”ì²­ ê°„ê²© ì¡°ì •
        if i < len(news_list):
            time.sleep(2)
    
    if filtered_count > 0:
        print(f"\nğŸ•˜ ì‹œê°„ í•„í„°ë¡œ ì œì™¸ëœ ë‰´ìŠ¤: {filtered_count}ê°œ")
        print(f"âœ… ìµœì¢… ì²˜ë¦¬ëœ ë‰´ìŠ¤: {len(processed_news)}ê°œ")
    
    return processed_news

def crawl_yakup_news(target_date=None):
    """ì•½ì—…ë‹·ì»´ ì˜ë£Œë‰´ìŠ¤ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
    
    # ê¸°ë³¸ê°’: ì˜¤ëŠ˜ ë‚ ì§œ (TEST_DATE í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
    if target_date is None:
        # TEST_DATE í™˜ê²½ë³€ìˆ˜ í™•ì¸
        import os
        test_date_env = os.getenv('TEST_DATE')
        if test_date_env:
            try:
                # TEST_DATE íŒŒì‹± (í˜•ì‹: 'YYYY-MM-DD')
                parsed_date = datetime.strptime(test_date_env, '%Y-%m-%d')
                target_date = parsed_date.strftime('%Y%m%d')
                print(f"ğŸ“… TEST_DATE ì‚¬ìš©: {test_date_env} -> {target_date}")
            except ValueError:
                print(f"âŒ ì˜ëª»ëœ TEST_DATE í˜•ì‹: {test_date_env}, í˜„ì¬ ë‚ ì§œ ì‚¬ìš©")
                target_date = datetime.now().strftime('%Y%m%d')
        else:
            target_date = datetime.now().strftime('%Y%m%d')
    
    # ë‚ ì§œ í˜•ì‹ ìœ íš¨ì„± ê²€ì‚¬ (YYYYMMDD í˜•ì‹)
    try:
        datetime.strptime(target_date, '%Y%m%d')
    except ValueError:
        print(f"âŒ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {target_date}. YYYYMMDD í˜•ì‹ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        return {
            'target_date': target_date,
            'target_url': '',
            'status': 'ì‹¤íŒ¨',
            'error': f'ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹: {target_date}',
            'news_count': 0,
            'news_list': [],
            'crawling_time': datetime.now().isoformat()
        }
    
    base_url = "http://m.yakup.com/news/index.html"
    target_url = f"{base_url}?cat=bclick&bc_date={target_date}"
    
    print("=" * 80)
    print("ğŸ“° ì•½ì—…ë‹·ì»´ ì˜ë£Œë‰´ìŠ¤ í¬ë¡¤ëŸ¬")
    print("=" * 80)
    print(f"ğŸ¯ ëŒ€ìƒ ë‚ ì§œ: {target_date}")
    print(f"ğŸŒ ì ‘ì† URL: {target_url}")
    
    driver = None
    news_data = []
    
    try:
        # Chrome ë“œë¼ì´ë²„ ì„¤ì •
        driver = setup_chrome_driver()
        if not driver:
            raise Exception("Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨")
        
        print("ğŸ“± ë©”ì¸ í˜ì´ì§€ ë¡œë”© ì¤‘...")
        driver.get(target_url)
        
        # ê¸°ë³¸ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        print("âœ… ë©”ì¸ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
        
        # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
        time.sleep(3)
        
        # 1ë‹¨ê³„: ë‰´ìŠ¤ URL ìˆ˜ì§‘
        news_urls = collect_news_urls(driver)
        
        if not news_urls:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ URLì´ ì—†ìŠµë‹ˆë‹¤")
            return {
                'target_date': target_date,
                'target_url': target_url,
                'status': 'ì„±ê³µ',
                'error': None,
                'news_count': 0,
                'news_list': []
            }
        
        # 2ë‹¨ê³„: ê° ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
        print(f"\nğŸ“Š {len(news_urls)}ê°œ ë‰´ìŠ¤ì˜ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹œì‘...")
        
        i = 1
        while i <= len(news_urls):
            news_url = news_urls[i-1]
            
            # 10ê°œë§ˆë‹¤ ë˜ëŠ” ì²« ì‹œì‘ ì‹œ ChromeDriver ì„¸ì…˜ ì´ˆê¸°í™”
            if (i-1) % 10 == 0:
                print(f"   ğŸ”„ ChromeDriver ì„¸ì…˜ ì´ˆê¸°í™” ({i}ë²ˆì§¸ ë‰´ìŠ¤)")
                
                # ê¸°ì¡´ ë“œë¼ì´ë²„ ì •ë¦¬
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
                
                # ìƒˆ ë“œë¼ì´ë²„ ìƒì„±
                driver = setup_chrome_driver()
                if not driver:
                    print(f"   âŒ ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨ - í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break
                print("   âœ… ChromeDriver ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ê°œë³„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œë„ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)
            max_attempts = 3
            success = False
            
            for attempt in range(1, max_attempts + 1):
                try:
                    print(f"ğŸ“° [{i}] ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ ì‹œë„ {attempt}/{max_attempts}...")
                    
                    # ì„¸ì…˜ ìƒíƒœ í™•ì¸
                    try:
                        driver.current_url  # ì„¸ì…˜ í™•ì¸
                    except Exception as session_error:
                        print(f"   âš ï¸ ChromeDriver ì„¸ì…˜ ë¬¸ì œ ê°ì§€: {session_error}")
                        
                        # ì„¸ì…˜ ì¬ìƒì„±
                        try:
                            driver.quit()
                        except:
                            pass
                        
                        driver = setup_chrome_driver()
                        if not driver:
                            print(f"   âŒ ChromeDriver ì¬ìƒì„± ì‹¤íŒ¨")
                            raise Exception("ChromeDriver ì¬ìƒì„± ì‹¤íŒ¨")
                        print("   âœ… ChromeDriver ì¬ìƒì„± ì™„ë£Œ")
                    
                    # ë‰´ìŠ¤ ìƒì„¸ ì •ë³´ í¬ë¡¤ë§
                    news_info = crawl_news_detail(driver, news_url, i)
                    
                    if news_info and news_info['title'] != "ì œëª© ì¶”ì¶œ ì‹¤íŒ¨":
                        news_data.append(news_info)
                        print(f"   âœ… ë‰´ìŠ¤ [{i}] ìˆ˜ì§‘ ì™„ë£Œ")
                        success = True
                        break
                    else:
                        raise Exception("ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                        
                except Exception as e:
                    print(f"   âŒ ë‰´ìŠ¤ [{i}] ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
                    
                    if attempt < max_attempts:
                        wait_time = attempt * 2
                        print(f"   â° {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                        time.sleep(wait_time)
                    else:
                        print(f"   ğŸ’¥ ë‰´ìŠ¤ [{i}] ìµœì¢… ì‹¤íŒ¨ - ë‹¤ìŒ ë‰´ìŠ¤ë¡œ ì§„í–‰")
            
            if not success:
                print(f"   âš ï¸ ë‰´ìŠ¤ [{i}] ìˆ˜ì§‘ ì‹¤íŒ¨ - ë‹¤ìŒìœ¼ë¡œ ì´ë™")
            
            # ë‹¤ìŒ ë‰´ìŠ¤ë¡œ ì´ë™
            i += 1
            
            # ìš”ì²­ ê°„ê²© ì¡°ì •
            if i <= len(news_urls):
                time.sleep(1)
        
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ: {len(news_data)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
        
        # 3ë‹¨ê³„: AI ìš”ì•½ ìƒì„± (ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ ì ìš©)
        if news_data:
            time_filter_enabled = should_filter_by_time_yakup()
            news_data = add_summaries(news_data, filter_enabled=time_filter_enabled)
        
        return {
            'target_date': target_date,
            'target_url': target_url,
            'status': 'ì„±ê³µ',
            'error': None,
            'news_count': len(news_data),
            'news_list': news_data,
            'crawling_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return {
            'target_date': target_date,
            'target_url': target_url,
            'status': 'ì‹¤íŒ¨',
            'error': str(e),
            'news_count': 0,
            'news_list': [],
            'crawling_time': datetime.now().isoformat()
        }
        
    finally:
        if driver:
            try:
                driver.quit()
                print("ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ")
            except:
                pass
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            if hasattr(driver, '_temp_dir'):
                try:
                    import shutil
                    shutil.rmtree(driver._temp_dir)
                    print(f"ğŸ§¹ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬: {driver._temp_dir}")
                except:
                    pass

def save_to_json(data, filename=None):
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    if filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        target_date = data.get('target_date', 'unknown')
        filename = f'medical_top_trending_news_{target_date}_{timestamp}.json'
    
    # crawler_result ë””ë ‰í† ë¦¬ì— ì €ì¥ - Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ ì‚¬ìš©
    result_dir = '/home/son/SKN12-FINAL-AIRFLOW/crawler_result'
    
    os.makedirs(result_dir, exist_ok=True)
    filepath = os.path.join(result_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
    return filepath

def main_with_retry(target_date=None, max_retries=3):
    """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ëœ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì‚¬ìš©ì ì…ë ¥ ë‚ ì§œê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ TEST_DATE ë˜ëŠ” ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©
    if target_date is None:
        # TEST_DATE í™˜ê²½ë³€ìˆ˜ í™•ì¸
        import os
        test_date_env = os.getenv('TEST_DATE')
        if test_date_env:
            try:
                # TEST_DATE íŒŒì‹± (í˜•ì‹: 'YYYY-MM-DD')
                parsed_date = datetime.strptime(test_date_env, '%Y-%m-%d')
                target_date = parsed_date.strftime('%Y%m%d')
                print(f"ğŸ“… TEST_DATE í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©: {test_date_env} -> {target_date}")
            except ValueError:
                print(f"âŒ ì˜ëª»ëœ TEST_DATE í˜•ì‹: {test_date_env}, í˜„ì¬ ë‚ ì§œ ì‚¬ìš©")
                target_date = datetime.now().strftime('%Y%m%d')
                print(f"ğŸ“… ë‚ ì§œê°€ ì§€ì •ë˜ì§€ ì•Šì•„ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {target_date}")
        else:
            target_date = datetime.now().strftime('%Y%m%d')
            print(f"ğŸ“… ë‚ ì§œê°€ ì§€ì •ë˜ì§€ ì•Šì•„ ì˜¤ëŠ˜ ë‚ ì§œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {target_date}")
    else:
        print(f"ğŸ“… ì§€ì •ëœ ë‚ ì§œë¡œ í¬ë¡¤ë§í•©ë‹ˆë‹¤: {target_date}")
    
    result = None
    for attempt in range(1, max_retries + 1):
        try:
            print(f"\nğŸ”„ í¬ë¡¤ë§ ì‹œë„ {attempt}/{max_retries}")
            
            # Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ (ì¬ì‹œë„ ì „)
            try:
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
                time.sleep(2)  # í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ëŒ€ê¸°
                print("ğŸ§¹ ì´ì „ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            except:
                pass
            
            # í¬ë¡¤ë§ ì‹¤í–‰
            result = crawl_yakup_news(target_date)
            
            # ì„±ê³µ ì—¬ë¶€ í™•ì¸
            if result['status'] == 'ì„±ê³µ' and result['news_count'] > 0:
                print(f"âœ… í¬ë¡¤ë§ ì„±ê³µ! ({attempt}/{max_retries})")
                break
            else:
                raise Exception(f"í¬ë¡¤ë§ ê²°ê³¼ ë¶ˆëŸ‰: {result.get('error', 'ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨')}")
                
        except KeyboardInterrupt:
            print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        except Exception as e:
            print(f"âŒ ì‹œë„ {attempt} ì‹¤íŒ¨: {e}")
            
            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„ ì•ˆë‚´
            if attempt < max_retries:
                wait_time = attempt * 5  # ì¬ì‹œë„ ê°„ê²©ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€
                print(f"â° {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                time.sleep(wait_time)
            else:
                print(f"ğŸ’¥ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨. ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries}) ë„ë‹¬")
                # ë§ˆì§€ë§‰ ì‹¤íŒ¨ ì‹œì—ë„ ê²°ê³¼ ê°ì²´ ìƒì„±
                if result is None:
                    result = {
                        'target_date': target_date,
                        'target_url': f"http://m.yakup.com/news/index.html?cat=bclick&bc_date={target_date}",
                        'status': 'ì‹¤íŒ¨',
                        'error': str(e),
                        'news_count': 0,
                        'news_list': [],
                        'crawling_time': datetime.now().isoformat()
                    }
        finally:
            # Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
            try:
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
            except:
                pass
    
    if result:
        # ê²°ê³¼ ì €ì¥
        filepath = save_to_json(result)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 80)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("=" * 80)
        print(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ: {result['target_date']}")
        print(f"ğŸŒ URL: {result['target_url']}")
        print(f"ğŸ“ˆ ìƒíƒœ: {result['status']}")
        print(f"ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {result['news_count']}ê°œ")
        
        if result['error']:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
        
        if result['news_list']:
            print("\nğŸ“‹ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡:")
            print("-" * 80)
            for news in result['news_list']:
                print(f"[{news['rank']:2d}] {news['title']}")
                if news['url']:
                    print(f"     ğŸ”— {news['url']}")
                if news['summary'] and news['summary'] not in ["", "ë³¸ë¬¸ ë‚´ìš© ë¶€ì¡±", "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"]:
                    print(f"     ğŸ“ {news['summary']}")
                print()
        
        print(f"ğŸ’¾ ê²°ê³¼ íŒŒì¼: {filepath}")
        if result['status'] == 'ì„±ê³µ':
            print("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
        else:
            print("ğŸ’¥ í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨!")


if __name__ == "__main__":
    # ë‚ ì§œ ì„¤ì • (TEST_DATE í™˜ê²½ë³€ìˆ˜ ìš°ì„ )
    import os
    test_date_env = os.getenv('TEST_DATE')
    if test_date_env:
        try:
            # TEST_DATE íŒŒì‹± (í˜•ì‹: 'YYYY-MM-DD')
            parsed_date = datetime.strptime(test_date_env, '%Y-%m-%d')
            target_date = parsed_date.strftime('%Y%m%d')
            print(f"ğŸ¯ TEST_DATEë¡œ ì„¤ì •ëœ ë‚ ì§œ: {test_date_env} -> {target_date}")
        except ValueError:
            print(f"âŒ ì˜ëª»ëœ TEST_DATE í˜•ì‹: {test_date_env}, í˜„ì¬ ë‚ ì§œ ì‚¬ìš©")
            target_date = datetime.now().strftime('%Y%m%d')
            print(f"ğŸ¯ ì„¤ì •ëœ ë‚ ì§œ: {target_date}")
    else:
        # ì˜¤ëŠ˜ ë‚ ì§œë¡œ í¬ë¡¤ë§
        target_date = datetime.now().strftime('%Y%m%d')
        print(f"ğŸ¯ ì„¤ì •ëœ ë‚ ì§œ: {target_date}")
    
    main_with_retry(target_date, max_retries=3)