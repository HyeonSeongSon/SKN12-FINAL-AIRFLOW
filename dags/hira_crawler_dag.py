#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HIRA ê³ ì‹œ ë°ì´í„° í¬ë¡¤ë§ Airflow DAG
ë§¤ì¼ 13:00ì— HIRA ê³ ì‹œ ë°ì´í„° ìˆ˜ì§‘
"""

from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
from datetime import datetime, timedelta
import os
import subprocess
import logging
import pendulum

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
local_tz = pendulum.timezone('Asia/Seoul')

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'hira-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 8, 6, 13, 0, tzinfo=local_tz),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
    'catchup': False
}

# DAG ì •ì˜
dag = DAG(
    'hira_crawler_daily',
    default_args=default_args,
    description='ë§¤ì¼ 13ì‹œì— HIRA ê³ ì‹œ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•˜ëŠ” DAG',
    schedule='0 13 * * *',
    max_active_runs=1,
    tags=['hira', 'crawler', 'daily']
)

def run_hira_crawler():
    """HIRA í¬ë¡¤ëŸ¬ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í¬ë¡¤ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
        crawler_script = '/opt/airflow/func/hira_crawler.py'
        
        logging.info(f"ğŸ¥ HIRA í¬ë¡¤ëŸ¬ ì‹œì‘: {crawler_script}")
        
        # Python ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ - ê°„ë‹¨í•œ ë°©ì‹
        env = os.environ.copy()
        process = subprocess.Popen(
            ['python3', crawler_script],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            cwd='/opt/airflow/func',
            env=env
        )
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (30ë¶„)
        try:
            stdout, stderr = process.communicate(timeout=1800)
            
            if process.returncode == 0:
                logging.info("âœ… HIRA í¬ë¡¤ë§ ì™„ë£Œ")
                logging.info(f"ì¶œë ¥: {stdout}")
                
                # ìƒì„±ëœ íŒŒì¼ í™•ì¸
                import glob
                # í¬ë¡¤ëŸ¬ê°€ ì‹¤ì œë¡œ ì €ì¥í•˜ëŠ” ê²½ë¡œì™€ ì¼ì¹˜ì‹œí‚´
                result_dir = '/home/son/SKN12-FINAL-AIRFLOW/crawler_result'
                json_files = glob.glob(os.path.join(result_dir, 'hira_data_*.json'))
                
                if json_files:
                    latest_file = max(json_files, key=os.path.getctime)
                    logging.info(f"ìƒì„±ëœ íŒŒì¼: {latest_file}")
                    return {'status': 'success', 'file': latest_file, 'output': stdout}
                else:
                    logging.warning("JSON íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    return {'status': 'warning', 'message': 'JSON íŒŒì¼ ì—†ìŒ', 'output': stdout}
            else:
                logging.error(f"âŒ HIRA í¬ë¡¤ë§ ì‹¤íŒ¨: {stderr}")
                raise RuntimeError(f"í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì‹¤íŒ¨: {stderr}")
                
        except subprocess.TimeoutExpired:
            process.terminate()
            logging.error("âŒ HIRA í¬ë¡¤ëŸ¬ íƒ€ì„ì•„ì›ƒ (30ë¶„)")
            raise RuntimeError("í¬ë¡¤ëŸ¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ")
            
    except Exception as e:
        logging.error(f"âŒ HIRA í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise

def check_and_notify(**context):
    """í¬ë¡¤ë§ ê²°ê³¼ í™•ì¸"""
    try:
        # ì´ì „ íƒœìŠ¤í¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        task_result = context['task_instance'].xcom_pull(task_ids='run_crawler')
        
        if task_result and task_result.get('status') == 'success':
            logging.info("âœ… HIRA í¬ë¡¤ë§ ì„±ê³µ")
            
            # ê²°ê³¼ íŒŒì¼ ì •ë³´
            result_file = task_result.get('file', 'Unknown')
            file_name = os.path.basename(result_file) if result_file != 'Unknown' else 'Unknown'
            
            logging.info(f"ğŸ“Š ì‹¤í–‰ ì •ë³´:")
            logging.info(f"- ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logging.info(f"- ìƒì„± íŒŒì¼: {file_name}")
            logging.info(f"- ëŒ€ìƒ: HIRA ê³ ì‹œ ë°ì´í„° (ì–´ì œ~ì˜¤ëŠ˜)")
            logging.info(f"- íŒŒì¼ ìœ„ì¹˜: {result_file}")
            logging.info("âœ… HIRA ê³ ì‹œ ë°ì´í„° í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            return {'status': 'success', 'file': result_file}
        else:
            logging.warning("âš ï¸ HIRA í¬ë¡¤ë§ ë¶€ë¶„ ì„±ê³µ ë˜ëŠ” ì‹¤íŒ¨")
            logging.warning(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logging.warning(f"ìƒíƒœ: {task_result.get('status', 'Unknown') if task_result else 'Failed'}")
            logging.warning(f"ë©”ì‹œì§€: {task_result.get('message', 'No message') if task_result else 'Task failed'}")
            
            return {'status': 'warning', 'message': task_result.get('message', 'No message') if task_result else 'Task failed'}
            
    except Exception as e:
        logging.error(f"âŒ ê²°ê³¼ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
        logging.error(f"ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        return {'status': 'error', 'message': str(e)}

def cleanup_chrome_processes():
    """Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬"""
    try:
        logging.info("ğŸ§¹ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì¤‘...")
        
        # Chrome ê´€ë ¨ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
        subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=10)
        subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=10)
        
        logging.info("âœ… Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        return {'status': 'success', 'message': 'Chrome processes cleaned up'}
        
    except Exception as e:
        logging.warning(f"âš ï¸ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {str(e)}")
        return {'status': 'warning', 'message': str(e)}

# Task ì •ì˜
crawler_task = PythonOperator(
    task_id='run_crawler',
    python_callable=run_hira_crawler,
    dag=dag,
    execution_timeout=timedelta(minutes=30)
)

check_task = PythonOperator(
    task_id='check_result',
    python_callable=check_and_notify,
    dag=dag,
)

cleanup_task = PythonOperator(
    task_id='cleanup_chrome',
    python_callable=cleanup_chrome_processes,
    dag=dag,
    trigger_rule='all_done'
)

# Task ì˜ì¡´ì„± ì„¤ì •
crawler_task >> check_task >> cleanup_task