#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ë‰´ìŠ¤ìŠ¤íƒ ë“œ iframe ê¸°ë°˜ KBS/MBC/SBS ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
ìš°ë¶„íˆ¬ í™˜ê²½ ëŒ€ì‘
"""

# ì‹¤ì‹œê°„ ì¶œë ¥ì„ ìœ„í•œ ì„¤ì •
import sys
sys.stdout.reconfigure(line_buffering=True)
sys.stderr.reconfigure(line_buffering=True)

import requests
import os
import subprocess
from dotenv import load_dotenv
from bs4 import BeautifulSoup
import openai
import re
import time
import json
from datetime import datetime, timedelta
from dateutil import parser
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# ì „ì—­ ì„¤ì •: ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ í›„ ë¡œê·¸ ìµœì†Œí™”
QUIET_MODE = True  # Trueë©´ ë¡œê·¸ ìµœì†Œí™”

def log_message(message, force=False, flush=True):
    """ì¡°ê±´ë¶€ ë¡œê·¸ ì¶œë ¥"""
    if not QUIET_MODE or force:
        print(message, flush=flush)

# .env íŒŒì¼ ë¡œë“œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ)
env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
load_dotenv(env_path)

def setup_chrome_driver_ubuntu():
    """ìš°ë¶„íˆ¬ í™˜ê²½ì— ìµœì í™”ëœ Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
    
    # ê¸°ì¡´ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
    try:
        import subprocess
        subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
        subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
        time.sleep(2)
        log_message("ğŸ§¹ ê¸°ì¡´ Chrome/ChromeDriver í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass
    
    try:
        chrome_options = Options()
        
        # Docker í™˜ê²½ì—ì„œëŠ” ë°˜ë“œì‹œ headless ëª¨ë“œ í•„ìš”
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-software-rasterizer')
        chrome_options.add_argument('--window-size=1280,720')
        
        # ì„¸ì…˜ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ í•µì‹¬ ì˜µì…˜ë“¤
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor,ChromeWhatsNewUI')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--disable-background-timer-throttling')
        chrome_options.add_argument('--disable-renderer-backgrounding')
        chrome_options.add_argument('--disable-backgrounding-occluded-windows')
        chrome_options.add_argument('--disable-client-side-phishing-detection')
        chrome_options.add_argument('--disable-component-extensions-with-background-pages')
        chrome_options.add_argument('--disable-ipc-flooding-protection')
        chrome_options.add_argument('--no-default-browser-check')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-background-networking')
        
        # ê³ ìœ  ì„¸ì…˜ì„ ìœ„í•œ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        import tempfile
        import uuid
        temp_dir = tempfile.mkdtemp(prefix=f'chrome_session_{uuid.uuid4().hex[:8]}_')
        chrome_options.add_argument(f'--user-data-dir={temp_dir}')
        
        # ê³ ìœ  ë””ë²„ê¹… í¬íŠ¸ ì„¤ì •
        import random
        debug_port = random.randint(9500, 9999)
        chrome_options.add_argument(f'--remote-debugging-port={debug_port}')
        
        # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”
        chrome_options.add_argument('--memory-pressure-off')
        chrome_options.add_argument('--max_old_space_size=2048')
        chrome_options.add_argument('--aggressive-cache-discard')
        
        # User Agent ì„¤ì •
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36')
        
        log_message(f"ğŸ”§ ì„ì‹œ ì„¸ì…˜ ë””ë ‰í† ë¦¬: {temp_dir}")
        log_message(f"ğŸ”§ ë””ë²„ê¹… í¬íŠ¸: {debug_port}")
        
        # Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” - Docker í™˜ê²½ì—ì„œëŠ” ì‹œìŠ¤í…œ ChromeDriver ì‚¬ìš©
        driver = None
        try:
            driver = webdriver.Chrome(options=chrome_options)
            log_message("âœ… Chrome ë“œë¼ì´ë²„ ìƒì„± ì„±ê³µ")
            
        except Exception as e:
            log_message(f"âŒ Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
        
        if driver:
            # íƒ€ì„ì•„ì›ƒ ë° ê¸°ë³¸ ì„¤ì •
            driver.implicitly_wait(10)
            driver.set_page_load_timeout(30)
            
            # ì„¸ì…˜ ì •ë³´ ì €ì¥ (ì •ë¦¬ìš©)
            driver._temp_dir = temp_dir
            driver._debug_port = debug_port
            
            log_message("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            return driver
        else:
            return None
                
    except Exception as e:
        log_message(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
        return None

def detect_current_press(driver):
    """í˜„ì¬ í™”ë©´ì— í‘œì‹œëœ ì–¸ë¡ ì‚¬ ê°ì§€ (ì •í™•í•œ xpath ê¸°ë°˜)"""
    try:
        # ì •í™•í•œ xpathë¡œ ì–¸ë¡ ì‚¬ ì´ë¯¸ì§€ ì°¾ê¸°
        press_img_xpath = "//*[@id='focusPanelCenter']/div/h3/a/img"
        target_presses = ['KBS', 'MBC', 'SBS']
        
        try:
            # í•´ë‹¹ ìœ„ì¹˜ì˜ ì´ë¯¸ì§€ ìš”ì†Œ ì°¾ê¸°
            press_img = driver.find_element(By.XPATH, press_img_xpath)
            
            if press_img.is_displayed():
                # alt ì†ì„± í™•ì¸
                alt_text = press_img.get_attribute('alt')
                log_message(f"ğŸ” ê°ì§€ëœ alt í…ìŠ¤íŠ¸: '{alt_text}'")
                
                # ëŒ€ìƒ ì–¸ë¡ ì‚¬ì¸ì§€ ì •í™•íˆ í™•ì¸ (ë¶€ë¶„ ë¬¸ìì—´ì´ ì•„ë‹Œ ì •í™•í•œ ë§¤ì¹­)
                for press in target_presses:
                    if press == 'KBS':
                        # KBSëŠ” ì •í™•íˆ 'KBS'ë§Œ ë§¤ì¹­ (KBS World ì œì™¸)
                        if alt_text == 'KBS' or alt_text.startswith('KBS ') or alt_text.endswith(' KBS') or ' KBS ' in alt_text:
                            # KBS WorldëŠ” ì œì™¸
                            if 'World' not in alt_text and 'world' not in alt_text:
                                log_message(f"ğŸ¯ í˜„ì¬ ì–¸ë¡ ì‚¬: KBS ê°ì§€ë¨ (alt: '{alt_text}')")
                                return 'KBS'
                    else:
                        # MBC, SBSëŠ” ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ
                        if press in alt_text:
                            log_message(f"ğŸ¯ í˜„ì¬ ì–¸ë¡ ì‚¬: {press} ê°ì§€ë¨ (alt: '{alt_text}')")
                            return press
                
                log_message(f"âŒ ëŒ€ìƒ ì–¸ë¡ ì‚¬ê°€ ì•„ë‹˜: {alt_text}")
                return None
            else:
                log_message("âŒ ì–¸ë¡ ì‚¬ ì´ë¯¸ì§€ê°€ í‘œì‹œë˜ì§€ ì•ŠìŒ")
                return None
                
        except NoSuchElementException:
            log_message(f"âŒ ì§€ì •ëœ xpathì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {press_img_xpath}")
            
            # ëŒ€ì²´ ë°©ë²•: ì¼ë°˜ì ì¸ alt ì†ì„± ê²€ìƒ‰
            log_message("ğŸ”„ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì–¸ë¡ ì‚¬ ê²€ìƒ‰ ì¤‘...")
            for press in target_presses:
                try:
                    if press == 'KBS':
                        # KBSëŠ” ì •í™•í•œ ë§¤ì¹­ìœ¼ë¡œ ê²€ìƒ‰
                        press_img = driver.find_element(By.XPATH, f"//img[@alt='KBS']")
                        if press_img.is_displayed():
                            log_message(f"ğŸ¯ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ KBS ê°ì§€ë¨")
                            return 'KBS'
                    else:
                        # MBC, SBSëŠ” ê¸°ì¡´ ë°©ì‹
                        press_img = driver.find_element(By.XPATH, f"//img[@alt='{press}']")
                        if press_img.is_displayed():
                            log_message(f"ğŸ¯ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ {press} ê°ì§€ë¨")
                            return press
                except NoSuchElementException:
                    continue
            
            log_message("âŒ ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œë„ ì–¸ë¡ ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
        
    except Exception as e:
        log_message(f"âŒ ì–¸ë¡ ì‚¬ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # í˜„ì¬ í˜ì´ì§€ ìƒíƒœ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        try:
            log_message(f"ğŸ“„ í˜„ì¬ URL: {driver.current_url}")
            log_message(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {driver.title}")
            
            # í˜ì´ì§€ì— ìˆëŠ” ëª¨ë“  img íƒœê·¸ì˜ alt ì†ì„± í™•ì¸
            all_imgs = driver.find_elements(By.TAG_NAME, "img")
            log_message(f"ğŸ“· í˜ì´ì§€ ë‚´ ì´ ì´ë¯¸ì§€ ìˆ˜: {len(all_imgs)}")
            
            for i, img in enumerate(all_imgs[:10]):  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸
                try:
                    alt = img.get_attribute('alt')
                    src = img.get_attribute('src')
                    if alt:
                        log_message(f"  [{i+1}] alt='{alt}', src='{src[:50]}...'")
                except:
                    continue
                    
        except Exception as debug_e:
            log_message(f"âŒ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥ ì‹¤íŒ¨: {debug_e}")
        
        return None

def extract_news_from_iframe(driver, press_name):
    """iframe ë‚´ë¶€ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ"""
    try:
        log_message(f"ğŸ“° {press_name} iframeì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ ì¤‘...")
        
        # iframe ì°¾ê¸°ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        iframe_selectors = [
            "//*[@id='focusPanelCenter']/div/iframe",
            "//iframe[contains(@src, 'newsstand')]",
            "//iframe",
            "#focusPanelCenter iframe",
            ".focus_panel iframe"
        ]
        
        iframe_found = False
        for selector in iframe_selectors:
            try:
                if selector.startswith("//") or selector.startswith("/"):
                    iframe = WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.XPATH, selector))
                    )
                else:
                    iframe = WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                    )
                
                # iframeìœ¼ë¡œ ì „í™˜
                driver.switch_to.frame(iframe)
                log_message(f"âœ… {press_name} iframe ì „í™˜ ì™„ë£Œ (ì„ íƒì: {selector})")
                iframe_found = True
                break
                
            except TimeoutException:
                continue
            except Exception as e:
                log_message(f"âŒ iframe ì„ íƒì '{selector}' ì˜¤ë¥˜: {e}")
                continue
        
        if not iframe_found:
            log_message(f"âŒ {press_name} iframeì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return []
            
        # iframe ë‚´ë¶€ í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        time.sleep(3)
        
        # iframe ë‚´ë¶€ í˜ì´ì§€ ìƒíƒœ í™•ì¸
        try:
            current_url = driver.current_url
            page_source_length = len(driver.page_source)
            log_message(f"ğŸ“„ iframe ë‚´ë¶€ URL: {current_url}")
            log_message(f"ğŸ“„ iframe í˜ì´ì§€ ì†ŒìŠ¤ ê¸¸ì´: {page_source_length}")
        except:
            pass
        
        headlines = []
        
        # iframe ë‚´ë¶€ì—ì„œ ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
        news_selectors = [
            "//a[contains(@href, 'news')]",
            # "//a[contains(@href, f'{press_name.lower()}')]",
            # "//div[contains(@class, 'news')]//a",
            # "//div[contains(@class, 'headline')]//a",
            # "//div[contains(@class, 'article')]//a",
            # "//ul//a",
            # "//li//a",
            # "//h1//a",
            # "//h2//a",
            # "//h3//a",
            # "//span//a",
            # "//p//a"
        ]
        
        for selector_index, selector in enumerate(news_selectors):
            try:
                log_message(f"  ğŸ” [{selector_index + 1}/{len(news_selectors)}] '{selector}' ì„ íƒì ê²€ìƒ‰ ì¤‘...")
                
                news_links = driver.find_elements(By.XPATH, selector)
                
                if not news_links:
                    log_message(f"    âŒ ë‰´ìŠ¤ ë§í¬ ì—†ìŒ")
                    continue
                
                log_message(f"    âœ… {len(news_links)}ê°œ ë§í¬ ë°œê²¬")
                
                for link in news_links[:20]:  # ê° ì„ íƒìë‹¹ ìµœëŒ€ 20ê°œ
                    try:
                        title = link.text.strip()
                        url = link.get_attribute('href')
                        
                        if not title or not url:
                            continue
                        
                        if len(title) < 5:  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸
                            continue
                        
                        # ì¤‘ë³µ ì²´í¬
                        if any(news['title'] == title for news in headlines):
                            continue
                        
                        # ë‰´ìŠ¤ ë°ì´í„° ìƒì„± (ë‚ ì§œëŠ” ë‚˜ì¤‘ì— ê°œë³„ ê¸°ì‚¬ì—ì„œ ì¶”ì¶œ)
                        news_data = {
                            'rank': len(headlines) + 1,
                            'title': title,
                            'url': url,
                            'press': press_name,
                            'pub_time': None,  # ê°œë³„ ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì¶”ì¶œí•  ì˜ˆì •
                            'source': f'iframe_{press_name.lower()}_selector_{selector_index + 1}',
                            'type': 'common news'
                        }
                        
                        headlines.append(news_data)
                        log_message(f"      [{len(headlines)}] {title[:50]}...")
                            
                    except Exception as e:
                        continue
                    
            except Exception as e:
                log_message(f"    âŒ '{selector}' ì„ íƒì ì˜¤ë¥˜: {e}")
                continue
        
        # iframeì—ì„œ ë²—ì–´ë‚˜ê¸°
        driver.switch_to.default_content()
        log_message(f"âœ… {press_name}ì—ì„œ {len(headlines)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        
        return headlines
        
    except Exception as e:
        log_message(f"âŒ {press_name} iframe ë‰´ìŠ¤ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        # iframeì—ì„œ ë²—ì–´ë‚˜ê¸°
        try:
            driver.switch_to.default_content()
        except:
            pass
        return []

def click_next_button(driver, max_retries=3):
    """ë‹¤ìŒ ë²„íŠ¼ í´ë¦­ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
    next_button_xpaths = [
        "//*[@id='content']/div[2]/div/div[4]/a[2]",
        "//a[contains(@class, 'next')]",
        "//a[contains(text(), 'ë‹¤ìŒ')]",
        "//button[contains(@class, 'next')]",
        "//div[contains(@class, 'paging')]//a[2]"
    ]
    
    for retry in range(max_retries):
        try:
            log_message(f"ğŸ”„ ë‹¤ìŒ ë²„íŠ¼ í´ë¦­ ì‹œë„ {retry + 1}/{max_retries}")
            
            # ì—¬ëŸ¬ xpath ì‹œë„
            next_button = None
            for xpath in next_button_xpaths:
                try:
                    next_button = WebDriverWait(driver, 3).until(
                        EC.element_to_be_clickable((By.XPATH, xpath))
                    )
                    log_message(f"âœ… ë²„íŠ¼ ë°œê²¬: {xpath}")
                    break
                except:
                    continue
            
            if next_button is None:
                log_message(f"âŒ {retry + 1}ë²ˆì§¸ ì‹œë„: ë‹¤ìŒ ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                continue
            
            # ë²„íŠ¼ í´ë¦­ ì‹œë„ (ë‘ ê°€ì§€ ë°©ë²•)
            try:
                # ë°©ë²• 1: JavaScript í´ë¦­
                driver.execute_script("arguments[0].click();", next_button)
                time.sleep(3)  # í˜ì´ì§€ ì „í™˜ ëŒ€ê¸° (3ì´ˆ)
                log_message("âœ… JavaScriptë¡œ ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                return True
            except Exception as e1:
                log_message(f"âŒ JavaScript í´ë¦­ ì‹¤íŒ¨: {e1}")
                try:
                    # ë°©ë²• 2: ì¼ë°˜ í´ë¦­
                    next_button.click()
                    time.sleep(3)  # í˜ì´ì§€ ì „í™˜ ëŒ€ê¸° (3ì´ˆ)
                    log_message("âœ… ì¼ë°˜ í´ë¦­ ì„±ê³µ")
                    return True
                except Exception as e2:
                    log_message(f"âŒ ì¼ë°˜ í´ë¦­ë„ ì‹¤íŒ¨: {e2}")
                    continue
                    
        except Exception as e:
            log_message(f"âŒ {retry + 1}ë²ˆì§¸ ì‹œë„ ì „ì²´ ì‹¤íŒ¨: {e}")
            if retry < max_retries - 1:
                log_message("ğŸ”„ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ì¬ì‹œë„...")
                try:
                    driver.refresh()
                    time.sleep(3)
                except:
                    pass
            continue
    
    log_message(f"âŒ {max_retries}ë²ˆ ì‹œë„ í›„ ë²„íŠ¼ í´ë¦­ ìµœì¢… ì‹¤íŒ¨")
    return False

def crawl_newsstand_with_iframe(driver):
    """iframe ê¸°ë°˜ ë‰´ìŠ¤ìŠ¤íƒ ë“œ í¬ë¡¤ë§ ë©”ì¸ í•¨ìˆ˜"""
    try:
        log_message("ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ìŠ¤íƒ ë“œ ì ‘ì† ì¤‘...")
        
        # í˜ì´ì§€ ì ‘ì† ì „ ëŒ€ê¸°
        time.sleep(2)
        
        # ë‰´ìŠ¤ìŠ¤íƒ ë“œ í˜ì´ì§€ ì ‘ì†
        driver.get("https://newsstand.naver.com/")
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë” ì•ˆì •ì ìœ¼ë¡œ)
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            log_message("âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
        except TimeoutException:
            log_message("âŒ í˜ì´ì§€ ë¡œë”© íƒ€ì„ì•„ì›ƒ")
            return []
        
        # ì¶”ê°€ ë¡œë”© ëŒ€ê¸°
        time.sleep(5)
        
        # í˜„ì¬ í˜ì´ì§€ ìƒíƒœ í™•ì¸
        log_message(f"ğŸ“„ í˜„ì¬ í˜ì´ì§€ ì œëª©: {driver.title}")
        log_message(f"ğŸ“„ í˜„ì¬ URL: {driver.current_url}")
        
        all_headlines = []
        found_presses = []  # KBS/MBC/SBS ìˆ˜ì§‘ ì™„ë£Œëœ ì–¸ë¡ ì‚¬
        all_seen_presses = []  # ëª¨ë“  ë³¸ ì–¸ë¡ ì‚¬ ê¸°ë¡ (ì¤‘ë³µ ê°ì§€ìš©)
        max_attempts = 200  # ìµœëŒ€ 200ë²ˆ ì‹œë„ë¡œ ì¦ê°€
        cycles_completed = 0  # ì™„ë£Œëœ ìˆœí™˜ íšŸìˆ˜
        first_press_in_cycle = None  # ê° ì‚¬ì´í´ì˜ ì²« ì–¸ë¡ ì‚¬
        
        for attempt in range(max_attempts):
            log_message(f"\nğŸ”„ {attempt + 1}ë²ˆì§¸ ì‹œë„...")
            
            # í˜„ì¬ ì–¸ë¡ ì‚¬ ê°ì§€
            current_press = detect_current_press(driver)
            
            if current_press:
                # ì‚¬ì´í´ ì‹œì‘ ê°ì§€
                if cycles_completed == 0 and first_press_in_cycle is None:
                    first_press_in_cycle = current_press
                    log_message(f"ğŸ ì²« ë²ˆì§¸ ì‚¬ì´í´ ì‹œì‘ - ì²« ì–¸ë¡ ì‚¬: {first_press_in_cycle}")
                
                # ì´ë¯¸ ë³¸ ì–¸ë¡ ì‚¬ê°€ ë‹¤ì‹œ ë‚˜íƒ€ë‚¬ëŠ”ì§€ í™•ì¸ (í•œ ì‚¬ì´í´ ì™„ë£Œ)
                if cycles_completed > 0 and current_press == first_press_in_cycle:
                    cycles_completed += 1
                    log_message(f"ğŸ”„ {cycles_completed}ë²ˆì§¸ ì‚¬ì´í´ ì™„ë£‰! (ë‹¤ì‹œ {first_press_in_cycle} ë“±ì¥)")
                    log_message(f"ğŸ“‹ ì´ë²ˆ ì‚¬ì´í´ì—ì„œ í™•ì¸í•œ ì–¸ë¡ ì‚¬: {all_seen_presses[-(len(all_seen_presses) % 52):] if len(all_seen_presses) > 52 else all_seen_presses}")
                    
                    # ì•„ì§ ëª» ì°¾ì€ ì–¸ë¡ ì‚¬ í™•ì¸
                    missing_presses = [p for p in ['KBS', 'MBC', 'SBS'] if p not in found_presses]
                    if missing_presses:
                        log_message(f"âš ï¸ ì•„ì§ ëª» ì°¾ì€ ì–¸ë¡ ì‚¬: {missing_presses}")
                        log_message("ğŸ”„ ë‹¤ìŒ ì‚¬ì´í´ ì§„í–‰...")
                    else:
                        log_message("âœ… ëª¨ë“  ëŒ€ìƒ ì–¸ë¡ ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!", force=True)
                        break
                
                # ì–¸ë¡ ì‚¬ ê¸°ë¡
                all_seen_presses.append(current_press)
                if cycles_completed == 0 and len(all_seen_presses) > 0 and current_press == all_seen_presses[0] and len(all_seen_presses) > 1:
                    cycles_completed = 1
                    log_message(f"ğŸ”„ ì²« ë²ˆì§¸ ì‚¬ì´í´ ì™„ë£‰ ê°ì§€!")
                
                # KBS/MBC/SBS ì¤‘ í•˜ë‚˜ì´ê³  ì•„ì§ ìˆ˜ì§‘í•˜ì§€ ì•Šì•˜ë‹¤ë©´ ë‰´ìŠ¤ ìˆ˜ì§‘
                target_presses = ['KBS', 'MBC', 'SBS']
                if current_press in target_presses and current_press not in found_presses:
                    log_message(f"ğŸ¯ ëŒ€ìƒ ì–¸ë¡ ì‚¬ ë°œê²¬: {current_press}", force=True)
                    
                    # iframeì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ
                    news_from_iframe = extract_news_from_iframe(driver, current_press)
                    
                    if news_from_iframe:
                        all_headlines.extend(news_from_iframe)
                        found_presses.append(current_press)
                        log_message(f"âœ… {current_press} ë‰´ìŠ¤ {len(news_from_iframe)}ê°œ ìˆ˜ì§‘ ì™„ë£‰", force=True)
                        log_message(f"ğŸ“Š í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘í•œ ì–¸ë¡ ì‚¬: {found_presses} ({len(found_presses)}/3)", force=True)
                    else:
                        log_message(f"âš ï¸ {current_press}ì—ì„œ ë‰´ìŠ¤ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", force=True)
                    
                    # 3ê°œ ì–¸ë¡ ì‚¬ ëª¨ë‘ ì°¾ì•˜ìœ¼ë©´ ì¢…ë£Œ
                    if len(found_presses) >= 3:
                        log_message("ğŸ‰ KBS, MBC, SBS ëª¨ë‘ ì°¾ì•˜ìŠµë‹ˆë‹¤!", force=True)
                        break
                elif current_press in target_presses:
                    log_message(f"â­ï¸ {current_press}ëŠ” ì´ë¯¸ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                else:
                    log_message(f"â­ï¸ {current_press}ëŠ” ëŒ€ìƒ ì–¸ë¡ ì‚¬ê°€ ì•„ë‹™ë‹ˆë‹¤.")
                
                # 2ì‚¬ì´í´ ì´ìƒ ëŒì•˜ëŠ”ë°ë„ ëª» ì°¾ì•˜ìœ¼ë©´ ê²½ê³ 
                if cycles_completed >= 2:
                    missing = [p for p in ['KBS', 'MBC', 'SBS'] if p not in found_presses]
                    if missing:
                        log_message(f"âš ï¸ {cycles_completed}ë²ˆì˜ ì‚¬ì´í´ í›„ì—ë„ {missing}ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", force=True)
            else:
                log_message("â­ï¸ ì–¸ë¡ ì‚¬ë¥¼ ê°ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ë‹¤ìŒ ë²„íŠ¼ í´ë¦­ (ì¬ì‹œë„ í¬í•¨)
            click_success = click_next_button(driver, max_retries=3)
            if not click_success:
                log_message("âš ï¸ ë‹¤ìŒ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨, í•˜ì§€ë§Œ íƒìƒ‰ ê³„ì†...")
                # ë²„íŠ¼ í´ë¦­ì— ì‹¤íŒ¨í•´ë„ íƒìƒ‰ì„ ê³„ì†í•˜ê¸° ìœ„í•´ ì§§ì€ ëŒ€ê¸° í›„ ì§„í–‰
                time.sleep(2)
                # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ë³µêµ¬ ì‹œë„
                try:
                    log_message("ğŸ”„ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ë³µêµ¬ ì‹œë„...")
                    driver.refresh()
                    time.sleep(3)
                except Exception as refresh_e:
                    log_message(f"âŒ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {refresh_e}")
                    # ê·¸ë˜ë„ ê³„ì† ì‹œë„
                    pass
        
        log_message(f"\nğŸ“Š ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼:", force=True)
        log_message(f"   ì‹œë„ íšŸìˆ˜: {attempt + 1}/{max_attempts}", force=True)
        log_message(f"   ì™„ë£Œëœ ì‚¬ì´í´ ìˆ˜: {cycles_completed}íšŒ", force=True)
        log_message(f"   í™•ì¸í•œ ê³ ìœ  ì–¸ë¡ ì‚¬ ìˆ˜: {len(set(all_seen_presses))}ê°œ", force=True)
        log_message(f"   ìˆ˜ì§‘ ì™„ë£Œëœ ì–¸ë¡ ì‚¬: {found_presses} ({len(found_presses)}/3)", force=True)
        log_message(f"   ì´ ë‰´ìŠ¤ ìˆ˜: {len(all_headlines)}ê°œ", force=True)
        
        # ëˆ„ë½ëœ ì–¸ë¡ ì‚¬ í‘œì‹œ
        missing = [p for p in ['KBS', 'MBC', 'SBS'] if p not in found_presses]
        if missing:
            log_message(f"   âš ï¸ ìˆ˜ì§‘í•˜ì§€ ëª»í•œ ì–¸ë¡ ì‚¬: {missing}", force=True)
        
        for press in ['KBS', 'MBC', 'SBS']:
            press_count = len([n for n in all_headlines if n['press'] == press])
            log_message(f"   - {press}: {press_count}ê°œ", force=True)
        
        return all_headlines
        
    except Exception as e:
        log_message(f"âŒ ë‰´ìŠ¤ìŠ¤íƒ ë“œ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}", force=True)
        return []

def extract_article_date_with_sbs_mbc(soup, url):
    """ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ (SBS/MBC ì „ìš© ë¡œì§ í¬í•¨)"""
    try:
        # MBC ë‰´ìŠ¤ì¸ì§€ í™•ì¸
        if 'mbc.co.kr' in url:
            log_message("    ğŸ¯ MBC ë‰´ìŠ¤ ê°ì§€ - MBC ì „ìš© ë‚ ì§œ ì¶”ì¶œ ì‹œì‘")
            
            # MBC ì „ìš© XPath ì„ íƒìë“¤ (CSS ì„ íƒìë¡œ ë³€í™˜)
            mbc_date_selectors = [
                # ìƒˆë¡œìš´ MBC ì „ìš© XPathë“¤
                '#content div section:nth-child(1) article div:nth-child(1) div:nth-child(3) div:nth-child(1)',
                '#content > div > section:nth-child(1) > article > div:nth-child(1) > div:nth-child(3) > div:nth-child(1)',
                '#wrap #container #content div section:nth-child(1) article div:nth-child(1) div:nth-child(3) div:nth-child(1)',
                # ì¶”ê°€ MBC ì„ íƒì
                '.article_date',
                '.date_area', 
                '.news_date',
                '.write-date',
                '.article-info .date',
                '.publish-date'
            ]
            
            for selector in mbc_date_selectors:
                try:
                    elem = soup.select_one(selector)
                    if elem:
                        text = elem.get_text(strip=True)
                        log_message(f"    ğŸ” MBC ë‚ ì§œ ìš”ì†Œ ë°œê²¬ ({selector}): '{text}'")
                        
                        # "ì…ë ¥"ê³¼ "ìˆ˜ì •" ë‘˜ ë‹¤ ìˆëŠ” ê²½ìš° "ì…ë ¥" ë‚ ì§œë§Œ ì¶”ì¶œ
                        if 'ì…ë ¥' in text:
                            # ì—¬ëŸ¬ ì¤„ì— ê±¸ì³ ìˆì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ì •ë¦¬
                            clean_text = re.sub(r'\s+', ' ', text.strip())
                            log_message(f"    ğŸ” MBC ì •ë¦¬ëœ í…ìŠ¤íŠ¸: '{clean_text}'")
                            
                            # "ì…ë ¥ 2025-08-13 06:50 | ìˆ˜ì • 2025-08-13 07:43" ë˜ëŠ” "ì…ë ¥ 2025-08-13 06:50" í˜•íƒœì—ì„œ ì…ë ¥ ë‚ ì§œë§Œ ì¶”ì¶œ
                            input_match = re.search(r'ì…ë ¥\s+(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})', clean_text)
                            if input_match:
                                date_text = input_match.group(1)
                                log_message(f"    âœ… MBC ì…ë ¥ ë‚ ì§œ ì¶”ì¶œ: '{date_text}'")
                                return parse_and_format_date(date_text)
                            
                            # ì‹œê°„ì´ ì—†ëŠ” ê²½ìš°: "ì…ë ¥ 2025-08-13"
                            input_match_no_time = re.search(r'ì…ë ¥\s+(\d{4}-\d{2}-\d{2})', clean_text)
                            if input_match_no_time:
                                date_text = input_match_no_time.group(1) + " 00:00"
                                log_message(f"    âœ… MBC ì…ë ¥ ë‚ ì§œ ì¶”ì¶œ (ì‹œê°„ ì—†ìŒ): '{date_text}'")
                                return parse_and_format_date(date_text)
                        
                        # "ì…ë ¥"ì´ ì—†ëŠ” ê²½ìš° ì¼ë°˜ì ì¸ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                        if re.search(r'\d{4}', text):
                            log_message(f"    ğŸ” MBC ì¼ë°˜ ë‚ ì§œ íŒ¨í„´ ì‹œë„: '{text}'")
                            return parse_and_format_date(text)
                            
                except Exception as e:
                    log_message(f"    âŒ MBC ì„ íƒì '{selector}' ì˜¤ë¥˜: {e}")
                    continue
            
            log_message("    âš ï¸ MBC ì „ìš© ì„ íƒìì—ì„œ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ì¼ë°˜ ë°©ì‹ìœ¼ë¡œ ì‹œë„")
        
        # SBS ë‰´ìŠ¤ì¸ì§€ í™•ì¸
        elif 'sbs.co.kr' in url:
            log_message("    ğŸ¯ SBS ë‰´ìŠ¤ ê°ì§€ - SBS ì „ìš© ë‚ ì§œ ì¶”ì¶œ ì‹œì‘")
            
            # SBS ì „ìš© XPath ì„ íƒìë“¤
            sbs_date_selectors = [
                # ì‚¬ìš©ìê°€ ì œê³µí•œ SBS ì „ìš© XPath (CSS ì„ íƒìë¡œ ë³€í™˜)
                '#container > div:nth-child(1) > div:nth-child(3) > div:nth-child(1) > div > div:nth-child(1) > div:nth-child(2)',
                '#container > div:nth-child(1) > div:nth-child(3) > div:nth-child(1) > div > div:nth-child(1)',
                # ì¶”ê°€ SBS ì„ íƒì
                '.article_date',
                '.date_area',
                '.news_date',
                '.write-date',
                '.article-info .date'
            ]
            
            for selector in sbs_date_selectors:
                try:
                    elem = soup.select_one(selector)
                    if elem:
                        text = elem.get_text(strip=True)
                        log_message(f"    ğŸ” SBS ë‚ ì§œ ìš”ì†Œ ë°œê²¬ ({selector}): '{text}'")
                        
                        # "ì‘ì„±"ê³¼ "ìˆ˜ì •" ë‘˜ ë‹¤ ìˆëŠ” ê²½ìš° "ì‘ì„±" ë‚ ì§œë§Œ ì¶”ì¶œ
                        if 'ì‘ì„±' in text:
                            # "ì‘ì„± 2024.08.12 14:30" í˜•íƒœì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œ
                            created_match = re.search(r'ì‘ì„±[^\d]*(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2}[^\d]*\d{1,2}:\d{2})', text)
                            if created_match:
                                date_text = created_match.group(1)
                                log_message(f"    âœ… SBS ì‘ì„± ë‚ ì§œ ì¶”ì¶œ: '{date_text}'")
                                return parse_and_format_date(date_text)
                            
                            # ì‹œê°„ì´ ì—†ëŠ” ê²½ìš°: "ì‘ì„± 2024.08.12"
                            created_match_no_time = re.search(r'ì‘ì„±[^\d]*(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})', text)
                            if created_match_no_time:
                                date_text = created_match_no_time.group(1) + " 00:00"
                                log_message(f"    âœ… SBS ì‘ì„± ë‚ ì§œ ì¶”ì¶œ (ì‹œê°„ ì—†ìŒ): '{date_text}'")
                                return parse_and_format_date(date_text)
                        
                        # "ì‘ì„±"ì´ ì—†ëŠ” ê²½ìš° ì¼ë°˜ì ì¸ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
                        if re.search(r'\d{4}', text):
                            log_message(f"    ğŸ” SBS ì¼ë°˜ ë‚ ì§œ íŒ¨í„´ ì‹œë„: '{text}'")
                            return parse_and_format_date(text)
                            
                except Exception as e:
                    log_message(f"    âŒ SBS ì„ íƒì '{selector}' ì˜¤ë¥˜: {e}")
                    continue
            
            log_message("    âš ï¸ SBS ì „ìš© ì„ íƒìì—ì„œ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ, ì¼ë°˜ ë°©ì‹ìœ¼ë¡œ ì‹œë„")
        
        # ì¼ë°˜ì ì¸ ë‚ ì§œ ì¶”ì¶œ (ê¸°ì¡´ ë¡œì§)
        return extract_article_date(soup)
        
    except Exception as e:
        log_message(f"    âŒ SBS ë‚ ì§œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return extract_article_date(soup)

def extract_article_date(soup):
    """ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ ë° YYYY:MM:DD hh:mm í˜•íƒœë¡œ í¬ë§·íŒ…"""
    try:
        # ë‹¤ì–‘í•œ ë‚ ì§œ ì„ íƒì íŒ¨í„´ ì‹œë„
        date_selectors = [
            # ë„¤ì´ë²„ ë‰´ìŠ¤
            'span.t11',
            'span._ARTICLE_DATE_TIME',
            'span.article_date',
            'span.date',
            'span.txt_date',
            'div.sponsor span.t11',
            'div.article_info span.t11',
            # KBS
            '.date',
            '.news-date',
            'span.datetime',
            'div.date-area span',
            '.byline .date',
            # MBC
            '.art-time',
            '.date-info',
            '.article-date',
            '.press-date',
            # SBS íŠ¹í™” ì„ íƒì ì¶”ê°€
            '.date_area',
            '.article_date',
            '.news_date',
            'div.date',
            '.art_date',
            '.article-info .date',
            '.news-info .date',
            'span.art-date-time',
            'div.art-date-time',
            '.publish-info .date',
            '.info-date',
            '.write-date',
            '.news-write-date',
            # ì¼ë°˜ì ì¸ íŒ¨í„´
            'time',
            '[datetime]',
            '.publish-date',
            '.published',
            '.timestamp',
            '.reg-date'
        ]
        
        date_text = None
        found_selector = None
        
        # CSS ì„ íƒìë¡œ ë‚ ì§œ ì°¾ê¸°
        for selector in date_selectors:
            try:
                elem = soup.select_one(selector)
                if elem:
                    # datetime ì†ì„± í™•ì¸
                    datetime_attr = elem.get('datetime')
                    if datetime_attr:
                        date_text = datetime_attr
                        found_selector = selector
                        log_message(f"    ğŸ” ë‚ ì§œ ë°œê²¬ (datetime ì†ì„±): {selector} -> {datetime_attr}")
                        break
                    
                    # í…ìŠ¤íŠ¸ ë‚´ìš© í™•ì¸
                    text = elem.get_text(strip=True)
                    if text and (re.search(r'\d{4}', text) or 'ì‹œê°„ ì „' in text or 'ë¶„ ì „' in text or 'ì¼ ì „' in text):
                        date_text = text
                        found_selector = selector
                        log_message(f"    ğŸ” ë‚ ì§œ ë°œê²¬ (í…ìŠ¤íŠ¸): {selector} -> {text}")
                        break
            except Exception as e:
                continue
        
        # ë©”íƒ€ íƒœê·¸ì—ì„œ ë‚ ì§œ ì°¾ê¸°
        if not date_text:
            meta_selectors = [
                'meta[property="article:published_time"]',
                'meta[name="article:published_time"]',
                'meta[property="og:article:published_time"]',
                'meta[name="pubdate"]',
                'meta[name="date"]',
                'meta[itemprop="datePublished"]'
            ]
            
            for selector in meta_selectors:
                try:
                    meta = soup.select_one(selector)
                    if meta:
                        content = meta.get('content')
                        if content:
                            date_text = content
                            break
                except:
                    continue
        
        if not date_text:
            # ë””ë²„ê¹…: í˜ì´ì§€ì—ì„œ ë‚ ì§œ ê´€ë ¨ ìš”ì†Œë“¤ í™•ì¸
            log_message("    ğŸ” ë‚ ì§œ ë””ë²„ê¹…: í˜ì´ì§€ ë‚´ ë‚ ì§œ ê´€ë ¨ ìš”ì†Œ ê²€ìƒ‰ ì¤‘...")
            
            # í˜ì´ì§€ ë‚´ ëª¨ë“  ì‹œê°„ ê´€ë ¨ ìš”ì†Œ ì°¾ê¸°
            debug_selectors = ['time', '[datetime]', '*[class*="date"]', '*[class*="time"]', '*[id*="date"]', '*[id*="time"]']
            
            for debug_sel in debug_selectors:
                try:
                    elems = soup.select(debug_sel)
                    for elem in elems[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                        text = elem.get_text(strip=True)
                        datetime_attr = elem.get('datetime')
                        class_attr = elem.get('class')
                        id_attr = elem.get('id')
                        
                        if text or datetime_attr:
                            log_message(f"      - {debug_sel}: text='{text}', datetime='{datetime_attr}', class={class_attr}, id={id_attr}")
                except:
                    continue
            
            # ë§ˆì§€ë§‰ ì‹œë„: í˜ì´ì§€ ì „ì²´ì—ì„œ ë‚ ì§œ íŒ¨í„´ ê²€ìƒ‰
            text_content = soup.get_text()
            date_patterns = [
                r'\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼\s*\d{1,2}ì‹œ\s*\d{1,2}ë¶„',
                r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}',
                r'\d{4}\.\d{2}\.\d{2}\s+\d{2}:\d{2}',
                r'\d{4}/\d{2}/\d{2}\s+\d{2}:\d{2}',
                r'\d{1,2}ì‹œê°„\s*ì „',
                r'\d{1,2}ë¶„\s*ì „',
                r'\d{1,2}ì¼\s*ì „'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, text_content)
                if match:
                    date_text = match.group()
                    log_message(f"    ğŸ” ì •ê·œì‹ìœ¼ë¡œ ë‚ ì§œ ë°œê²¬: {pattern} -> {date_text}")
                    break
        
        if not date_text:
            log_message("    âš ï¸ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None
        
        # ë‚ ì§œ íŒŒì‹± ë° í¬ë§·íŒ…
        log_message(f"    ğŸ“… ë‚ ì§œ íŒŒì‹± ì‹œë„: '{date_text}' (ì„ íƒì: {found_selector})")
        return parse_and_format_date(date_text)
        
    except Exception as e:
        log_message(f"    âŒ ë‚ ì§œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

def parse_and_format_date(date_text):
    """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±í•˜ì—¬ YYYY.MM.DD hh:mm í˜•íƒœë¡œ ë³€í™˜"""
    try:
        current_time = datetime.now()
        
        # ìƒëŒ€ì  ì‹œê°„ í‘œí˜„ ì²˜ë¦¬
        if 'ì‹œê°„ ì „' in date_text:
            hours_ago = int(re.search(r'(\d+)ì‹œê°„', date_text).group(1))
            target_time = current_time - timedelta(hours=hours_ago)
            return target_time.strftime("%Y.%m.%d %H:%M")
        
        elif 'ë¶„ ì „' in date_text:
            minutes_ago = int(re.search(r'(\d+)ë¶„', date_text).group(1))
            target_time = current_time - timedelta(minutes=minutes_ago)
            return target_time.strftime("%Y.%m.%d %H:%M")
        
        elif 'ì¼ ì „' in date_text:
            days_ago = int(re.search(r'(\d+)ì¼', date_text).group(1))
            target_time = current_time - timedelta(days=days_ago)
            return target_time.strftime("%Y.%m.%d %H:%M")
        
        # SBS íŠ¹ìˆ˜ í˜•ì‹ ì²˜ë¦¬: "2024.8.12 ì˜¤í›„ 2:30" ë˜ëŠ” "2024.8.12 14:30"
        elif re.search(r'\d{4}\.\d{1,2}\.\d{1,2}', date_text):
            try:
                # ì˜¤ì „/ì˜¤í›„ í‘œí˜„ì´ ìˆëŠ” ê²½ìš°
                am_pm_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})\s*(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2}):(\d{2})', date_text)
                if am_pm_match:
                    year, month, day, am_pm, hour, minute = am_pm_match.groups()
                    hour = int(hour)
                    
                    # ì˜¤í›„ì¸ ê²½ìš° 12ì‹œê°„ ì¶”ê°€ (12ì‹œëŠ” ì˜ˆì™¸)
                    if am_pm == 'ì˜¤í›„' and hour != 12:
                        hour += 12
                    elif am_pm == 'ì˜¤ì „' and hour == 12:
                        hour = 0
                    
                    return f"{year}.{month.zfill(2)}.{day.zfill(2)} {hour:02d}:{minute}"
                
                # ì‹œê°„ì´ ì—†ëŠ” ê²½ìš°: "2024.8.12"
                date_only_match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})', date_text)
                if date_only_match:
                    year, month, day = date_only_match.groups()
                    return f"{year}.{month.zfill(2)}.{day.zfill(2)} 00:00"
                    
            except Exception as e:
                log_message(f"    âŒ SBS ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        # í•œêµ­ì–´ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        elif 'ë…„' in date_text and 'ì›”' in date_text and 'ì¼' in date_text:
            # "2024ë…„ 1ì›” 15ì¼ 14ì‹œ 30ë¶„" í˜•íƒœ
            date_match = re.search(r'(\d{4})ë…„ (\d{1,2})ì›” (\d{1,2})ì¼(?:\s+(\d{1,2})ì‹œ\s+(\d{1,2})ë¶„)?', date_text)
            if date_match:
                year, month, day = date_match.group(1), date_match.group(2), date_match.group(3)
                hour = date_match.group(4) if date_match.group(4) else "00"
                minute = date_match.group(5) if date_match.group(5) else "00"
                return f"{year}.{month.zfill(2)}.{day.zfill(2)} {hour.zfill(2)}:{minute.zfill(2)}"
        
        # ISO 8601 í˜•ì‹ ì²˜ë¦¬
        elif 'T' in date_text:
            try:
                dt = parser.parse(date_text)
                return dt.strftime("%Y.%m.%d %H:%M")
            except:
                pass
        
        # ì¼ë°˜ì ì¸ ë‚ ì§œ í˜•ì‹ë“¤ ì‹œë„
        date_formats = [
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%d %H:%M',
            '%Y.%m.%d %H:%M',
            '%Y/%m/%d %H:%M',
            '%Y-%m-%d',
            '%Y.%m.%d',
            '%Y/%m/%d'
        ]
        
        for fmt in date_formats:
            try:
                dt = datetime.strptime(date_text.strip(), fmt)
                return dt.strftime("%Y.%m.%d %H:%M")
            except ValueError:
                continue
        
        # dateutil.parser ì‚¬ìš© (ë§ˆì§€ë§‰ ì‹œë„)
        try:
            dt = parser.parse(date_text, fuzzy=True)
            return dt.strftime("%Y.%m.%d %H:%M")
        except:
            pass
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
        log_message(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, í˜„ì¬ ì‹œê°„ ì‚¬ìš©: '{date_text}'")
        return current_time.strftime("%Y.%m.%d %H:%M")
        
    except Exception as e:
        log_message(f"    âŒ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}")
        return datetime.now().strftime("%Y.%m.%d %H:%M")

def crawl_article_content(url):
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ë° ë‚ ì§œ ì¶”ì¶œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ (SBS/MBC ì „ìš© ë¡œì§ í¬í•¨)
        article_date = extract_article_date_with_sbs_mbc(soup, url)
        
        # ë‹¤ì–‘í•œ ë³¸ë¬¸ ì„ íƒì ì‹œë„
        content_selectors = [
            'div#articleBodyContents',
            'div.news_end',
            'div.article_body',
            'div._article_body_contents',
            'div.go_trans._article_content',
            'article',
            'div.article-view-content-div',
            'div#newsEndContents',
            'div.article_body_text',
            'div.article-body',
            '.article_content'
        ]
        
        article_content = ""
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                for script in content_elem(["script", "style"]):
                    script.decompose()
                article_content = content_elem.get_text(strip=True)
                break
        
        if not article_content:
            paragraphs = soup.find_all('p')
            article_content = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        article_content = re.sub(r'\s+', ' ', article_content)
        article_content = re.sub(r'\n+', ' ', article_content)
        
        return {
            'content': article_content.strip() if article_content else None,
            'pub_date': article_date
        }
        
    except Exception as e:
        log_message(f"    âŒ ë³¸ë¬¸/ë‚ ì§œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return {
            'content': None,
            'pub_date': None
        }

def summarize_with_llm(content, title, press):
    """OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ ìš”ì•½"""
    if not content or len(content.strip()) < 50:
        return None
    
    try:
        if len(content) > 3000:
            content = content[:3000] + "..."
        
        client = openai.OpenAI()
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system", 
                    "content": f"ë‹¹ì‹ ì€ {press} ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ìš”ì•½í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 2-3ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ìš”ì•½í•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user", 
                    "content": f"ë‹¤ìŒ {press} ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\nì œëª©: {title}\n\në‚´ìš©: {content}"
                }
            ],
            max_tokens=200,
            temperature=0.3
        )
        
        summary = response.choices[0].message.content.strip()
        return summary
        
    except Exception as e:
        log_message(f"    âŒ LLM ìš”ì•½ ì˜¤ë¥˜: {e}")
        return None

def should_filter_by_time():
    """í˜„ì¬ ì‹œê°„ì— ë”°ë¥¸ ë‰´ìŠ¤ í•„í„°ë§ ì—¬ë¶€ ê²°ì •"""
    current_hour = datetime.now().hour
    
    if current_hour >= 13:
        log_message(f"â° í˜„ì¬ ì‹œê°: {current_hour}ì‹œ - 09ì‹œ ì´í›„ ë‰´ìŠ¤ë§Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤.", force=True)
        return True
    else:
        log_message(f"â° í˜„ì¬ ì‹œê°: {current_hour}ì‹œ - ëª¨ë“  ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.", force=True)
        return False

def is_news_time_valid(news_pub_time, filter_enabled):
    """ë‰´ìŠ¤ ë°œí–‰ ì‹œê°„ì´ í•„í„°ë§ ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸"""
    if not filter_enabled or not news_pub_time:
        return True
    
    try:
        # YYYY.MM.DD hh:mm í˜•ì‹ íŒŒì‹±
        news_datetime = datetime.strptime(news_pub_time, "%Y.%m.%d %H:%M")
        today = datetime.now().date()
        
        # ì˜¤ëŠ˜ ë‚ ì§œì¸ ê²½ìš°ë§Œ ì‹œê°„ í•„í„° ì ìš©
        if news_datetime.date() == today:
            if news_datetime.hour >= 9:
                log_message(f"    âœ… ì‹œê°„ í•„í„° í†µê³¼: {news_pub_time} (09ì‹œ ì´í›„)")
                return True
            else:
                log_message(f"    âŒ ì‹œê°„ í•„í„° ì œì™¸: {news_pub_time} (09ì‹œ ì´ì „)")
                return False
        else:
            # ì˜¤ëŠ˜ì´ ì•„ë‹Œ ë‚ ì§œëŠ” ëª¨ë‘ í¬í•¨
            log_message(f"    âœ… ë‚ ì§œ í•„í„° í†µê³¼: {news_pub_time} (ì˜¤ëŠ˜ì´ ì•„ë‹Œ ë‚ ì§œ)")
            return True
            
    except Exception as e:
        log_message(f"    âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜, ë‰´ìŠ¤ í¬í•¨: {news_pub_time} - {e}")
        return True

def save_news_data(news_list, filename=None):
    """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"newsstand_iframe_{timestamp}.json"
    
    # crawler_result ë””ë ‰í† ë¦¬ì— ì €ì¥ - Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ ì‚¬ìš©
    result_dir = '/home/son/SKN12-FINAL-AIRFLOW/crawler_result'
    
    os.makedirs(result_dir, exist_ok=True)
    filepath = os.path.join(result_dir, filename)
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        log_message(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£‰: {filepath}", force=True)
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if os.path.exists(filepath):
            log_message(f"âœ… íŒŒì¼ í™•ì¸ë¨: {os.path.getsize(filepath)} bytes", force=True)
        return filepath
    except Exception as e:
        log_message(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}", force=True)
        log_message(f"    ì‹œë„í•œ ê²½ë¡œ: {filepath}", force=True)
        return None

def main_with_retry(max_retries=3):
    """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ëœ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    log_message("=" * 80, force=True)
    log_message("ğŸ“º ë„¤ì´ë²„ ë‰´ìŠ¤ìŠ¤íƒ ë“œ iframe ê¸°ë°˜ KBS/MBC/SBS ë‰´ìŠ¤ í¬ë¡¤ëŸ¬", force=True)
    log_message("ğŸ§ ìš°ë¶„íˆ¬ í™˜ê²½ ìµœì í™” + 3íšŒ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜", force=True)
    log_message("=" * 80, force=True)
    
    # OpenAI API í‚¤ í™•ì¸
    if not os.getenv('OPENAI_API_KEY'):
        log_message("âŒ í™˜ê²½ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", force=True)
        log_message("ğŸ“ .env íŒŒì¼ì— ë‹¤ìŒ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”:", force=True)
        log_message("   OPENAI_API_KEY=your_openai_api_key", force=True)
        return
    
    headlines = []
    
    for attempt in range(1, max_retries + 1):
        driver = None
        try:
            log_message(f"\nğŸ”„ ë‰´ìŠ¤ìŠ¤íƒ ë“œ í¬ë¡¤ë§ ì‹œë„ {attempt}/{max_retries}", force=True)
            
            # Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ (ì¬ì‹œë„ ì „)
            try:
                import subprocess
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
                time.sleep(2)
                log_message("ğŸ§¹ ì´ì „ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            except:
                pass
            
            # ìƒˆë¡œìš´ ë“œë¼ì´ë²„ ì„¤ì •
            log_message(f"ğŸš€ 1ë‹¨ê³„: Chrome ë¸Œë¼ìš°ì € ì„¤ì • ì¤‘... (ì‹œë„ {attempt}/{max_retries})", force=True)
            driver = setup_chrome_driver_ubuntu()
            if not driver:
                raise Exception("Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨")
            
            # ë‰´ìŠ¤ìŠ¤íƒ ë“œ í¬ë¡¤ë§
            log_message(f"ğŸ“° 2ë‹¨ê³„: iframe ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘... (ì‹œë„ {attempt}/{max_retries})", force=True)
            headlines = crawl_newsstand_with_iframe(driver)
            
            if not headlines:
                raise Exception("ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: í—¤ë“œë¼ì¸ ì—†ìŒ")
            
            # í¬ë¡¤ë§ ê²°ê³¼ ê²€ì¦
            found_presses = list(set([news['press'] for news in headlines]))
            required_presses = ['KBS', 'MBC', 'SBS']
            missing_presses = [p for p in required_presses if p not in found_presses]
            
            log_message(f"âœ… ì´ {len(headlines)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.", force=True)
            log_message(f"ğŸ“Š ìˆ˜ì§‘ëœ ì–¸ë¡ ì‚¬: {found_presses}", force=True)
            
            # ì„±ê³µ ì¡°ê±´: ìµœì†Œ 2ê°œ ì–¸ë¡ ì‚¬ ë˜ëŠ” ë§ˆì§€ë§‰ ì‹œë„ì—ì„œëŠ” 1ê°œ ì´ìƒ
            min_required = 2 if attempt < max_retries else 1
            if len(found_presses) >= min_required:
                log_message(f"âœ… ë‰´ìŠ¤ìŠ¤íƒ ë“œ í¬ë¡¤ë§ ì„±ê³µ! ({attempt}/{max_retries})", force=True)
                if missing_presses:
                    log_message(f"âš ï¸ ì¼ë¶€ ëˆ„ë½ëœ ì–¸ë¡ ì‚¬: {missing_presses}", force=True)
                break
            else:
                raise Exception(f"ë¶ˆì¶©ë¶„í•œ ì–¸ë¡ ì‚¬ ìˆ˜ì§‘: {found_presses} (ìµœì†Œ {min_required}ê°œ í•„ìš”)")
                
        except Exception as e:
            log_message(f"âŒ ì‹œë„ {attempt} ì‹¤íŒ¨: {e}", force=True)
            
            # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„ ì•ˆë‚´
            if attempt < max_retries:
                wait_time = attempt * 5  # ì¬ì‹œë„ ê°„ê²©ì„ ì ì§„ì ìœ¼ë¡œ ì¦ê°€
                log_message(f"â° {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...", force=True)
                time.sleep(wait_time)
            else:
                log_message(f"ğŸ’¥ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨. ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries}) ë„ë‹¬", force=True)
                headlines = []  # ë¹ˆ ê²°ê³¼ ì„¤ì •
        
        finally:
            # ê° ì‹œë„ë§ˆë‹¤ ë“œë¼ì´ë²„ ì •ë¦¬
            if driver:
                try:
                    driver.quit()
                    log_message(f"ğŸ”š ë¸Œë¼ìš°ì € ì¢…ë£Œ (ì‹œë„ {attempt})")
                    time.sleep(2)
                except:
                    pass
            
            # Chrome í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì •ë¦¬
            try:
                import subprocess
                subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
            except:
                pass
    
    # ê²°ê³¼ ì²˜ë¦¬
    if not headlines:
        log_message("ğŸ’¥ ë‰´ìŠ¤ìŠ¤íƒ ë“œ í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨!", force=True)
        return
        
    return headlines

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)"""
    headlines = main_with_retry(max_retries=3)
    
    if not headlines:
        log_message("ğŸ’¥ ë‰´ìŠ¤ìŠ¤íƒ ë“œ í¬ë¡¤ë§ ìµœì¢… ì‹¤íŒ¨!", force=True)
        return
        
    # 3ë‹¨ê³„: ë³¸ë¬¸ ì¶”ì¶œ ë° ìš”ì•½
    log_message(f"\nğŸ“ 3ë‹¨ê³„: ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ ë° AI ìš”ì•½ ìƒì„± ì¤‘...", force=True)
    
    # ì‹œê°„ ê¸°ë°˜ í•„í„°ë§ ì¡°ê±´ í™•ì¸
    time_filter_enabled = should_filter_by_time()
    
    processed_news = []
    filtered_count = 0
    
    for i, news in enumerate(headlines, 1):
        print(f"\n[{i:2d}/{len(headlines)}] {news['press']} - {news['title'][:60]}...", flush=True)
        
        # ë³¸ë¬¸ ë° ë‚ ì§œ ì¶”ì¶œ
        print("    ğŸ“„ ë³¸ë¬¸ ë° ë‚ ì§œ ì¶”ì¶œ ì¤‘...", flush=True)
        article_data = crawl_article_content(news['url'])
        
        if article_data['content']:
            print(f"    âœ… ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ (ê¸¸ì´: {len(article_data['content'])}ì)", flush=True)
            
            # ë‚ ì§œ ì—…ë°ì´íŠ¸
            if article_data['pub_date']:
                news['pub_time'] = article_data['pub_date']
                print(f"    ğŸ“… ì—…ë¡œë“œ ë‚ ì§œ: {article_data['pub_date']}", flush=True)
            else:
                news['pub_time'] = datetime.now().strftime("%Y.%m.%d %H:%M")
                print("    âš ï¸ ë‚ ì§œ ì¶”ì¶œ ì‹¤íŒ¨, í˜„ì¬ ì‹œê°„ ì‚¬ìš©", flush=True)
            
            # ì‹œê°„ í•„í„°ë§ ê²€ì‚¬
            if not is_news_time_valid(news['pub_time'], time_filter_enabled):
                filtered_count += 1
                print(f"    ğŸš« ì‹œê°„ ì¡°ê±´ìœ¼ë¡œ ì¸í•´ ì œì™¸ë¨", flush=True)
                continue
            
            # LLM ìš”ì•½
            print("    ğŸ¤– AI ìš”ì•½ ìƒì„± ì¤‘...", flush=True)
            summary = summarize_with_llm(article_data['content'], news['title'], news['press'])
            
            if summary:
                print(f"    ğŸ“ AI ìš”ì•½: {summary}", flush=True)
                news['ai_summary'] = summary
            else:
                print("    âŒ AI ìš”ì•½ ì‹¤íŒ¨", flush=True)
                news['ai_summary'] = None
        else:
            print("    âŒ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨", flush=True)
            news['pub_time'] = datetime.now().strftime("%Y.%m.%d %H:%M")
            news['ai_summary'] = None
            
            # ì‹œê°„ í•„í„°ë§ ê²€ì‚¬ (ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨í•œ ê²½ìš°ë„)
            if not is_news_time_valid(news['pub_time'], time_filter_enabled):
                filtered_count += 1
                print(f"    ğŸš« ì‹œê°„ ì¡°ê±´ìœ¼ë¡œ ì¸í•´ ì œì™¸ë¨", flush=True)
                continue
        
        processed_news.append(news)
        
        # API ì œí•œ ê³ ë ¤í•œ ëŒ€ê¸°
        if i < len(headlines):
            time.sleep(2)
    
    # 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    print(f"\nğŸ’¾ 4ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ì¤‘...", flush=True)
    filename = save_news_data(processed_news)
    
    # 5ë‹¨ê³„: ìš”ì•½ ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ“‹ 5ë‹¨ê³„: ìµœì¢… ê²°ê³¼", flush=True)
    print("=" * 80, flush=True)
    
    success_count = sum(1 for news in processed_news if news.get('ai_summary'))
    
    print(f"ğŸ“Š í¬ë¡¤ë§ëœ ë‰´ìŠ¤: {len(headlines)}ê°œ", flush=True)
    print(f"ğŸ•˜ ì‹œê°„ í•„í„°ë¡œ ì œì™¸ëœ ë‰´ìŠ¤: {filtered_count}ê°œ", flush=True)
    print(f"âœ… ìµœì¢… ì²˜ë¦¬ëœ ë‰´ìŠ¤: {len(processed_news)}ê°œ", flush=True)
    print(f"ğŸ“ ìš”ì•½ ì„±ê³µ: {success_count}ê°œ", flush=True)
    print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {len(processed_news) - success_count}ê°œ", flush=True)
    
    for press in ['KBS', 'MBC', 'SBS']:
        press_news = [n for n in processed_news if n['press'] == press]
        press_summaries = [n for n in press_news if n.get('ai_summary')]
        print(f"ğŸ“º {press}: {len(press_news)}ê°œ (ìš”ì•½ ì™„ë£Œ: {len(press_summaries)}ê°œ)", flush=True)
    
    print(f"\nğŸ’¾ ì €ì¥ëœ íŒŒì¼: {filename}", flush=True)
    
    print("\nğŸ“º ë°©ì†¡3ì‚¬ ë‰´ìŠ¤ ìš”ì•½:", flush=True)
    print("-" * 80, flush=True)
    
    for i, news in enumerate(processed_news, 1):
        print(f"\n[{i:2d}] {news['press']} - {news['title']}", flush=True)
        if news.get('ai_summary'):
            print(f"    ğŸ“ {news['ai_summary']}", flush=True)
        else:
            print(f"    âŒ ìš”ì•½ ì—†ìŒ", flush=True)
    
    print("\n" + "=" * 80, flush=True)
    print("ğŸ‰ ë„¤ì´ë²„ ë‰´ìŠ¤ìŠ¤íƒ ë“œ iframe í¬ë¡¤ë§ ì™„ë£Œ!", flush=True)

def cleanup_resources():
    """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ í•¨ìˆ˜"""
    try:
        import subprocess
        import shutil
        import glob
        
        # Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
        subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
        subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        temp_dirs = glob.glob('/tmp/chrome_session_*')
        for temp_dir in temp_dirs:
            try:
                shutil.rmtree(temp_dir)
                print(f"ğŸ§¹ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬: {temp_dir}", flush=True)
            except:
                pass
                
        print("ğŸ§¹ Chrome í”„ë¡œì„¸ìŠ¤ ë° ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ", flush=True)
    except:
        pass

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.", flush=True)
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
        import traceback
        traceback.print_exc()
    finally:
        cleanup_resources()
