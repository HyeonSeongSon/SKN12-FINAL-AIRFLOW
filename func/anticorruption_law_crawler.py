#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import time
import requests
from datetime import datetime
from bs4 import BeautifulSoup
import os
import sys
import tempfile
import uuid
import random
import subprocess
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

def setup_chrome_driver_ubuntu():
    """ìš°ë¶„íˆ¬ í™˜ê²½ì— ìµœì í™”ëœ Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
    
    # ê¸°ì¡´ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
    try:
        subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
        subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
        time.sleep(2)
        print("ğŸ§¹ ê¸°ì¡´ Chrome/ChromeDriver í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
    except:
        pass
    
    try:
        chrome_options = Options()
        
        # Docker í™˜ê²½ì—ì„œëŠ” ë°˜ë“œì‹œ headless ëª¨ë“œ í•„ìš”
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--disable-software-rasterizer')
        chrome_options.add_argument('--window-size=1280,720')
        
        # ì„¸ì…˜ ì¶©ëŒ ë°©ì§€ë¥¼ ìœ„í•œ í•µì‹¬ ì˜µì…˜ë“¤
        chrome_options.add_argument('--disable-web-security')
        chrome_options.add_argument('--disable-features=VizDisplayCompositor,ChromeWhatsNewUI')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-default-apps')
        chrome_options.add_argument('--disable-sync')
        chrome_options.add_argument('--disable-background-timer-throttling')
        chrome_options.add_argument('--disable-renderer-backgrounding')
        chrome_options.add_argument('--disable-backgrounding-occluded-windows')
        chrome_options.add_argument('--disable-client-side-phishing-detection')
        chrome_options.add_argument('--disable-component-extensions-with-background-pages')
        chrome_options.add_argument('--disable-ipc-flooding-protection')
        chrome_options.add_argument('--no-default-browser-check')
        chrome_options.add_argument('--no-first-run')
        chrome_options.add_argument('--disable-background-networking')
        chrome_options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ í–¥ìƒ
        
        # ê³ ìœ  ì„¸ì…˜ì„ ìœ„í•œ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        temp_dir = tempfile.mkdtemp(prefix=f'chrome_session_{uuid.uuid4().hex[:8]}_')
        chrome_options.add_argument(f'--user-data-dir={temp_dir}')
        
        # ê³ ìœ  ë””ë²„ê¹… í¬íŠ¸ ì„¤ì •
        debug_port = random.randint(9500, 9999)
        chrome_options.add_argument(f'--remote-debugging-port={debug_port}')
        
        # ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”
        chrome_options.add_argument('--memory-pressure-off')
        chrome_options.add_argument('--max_old_space_size=2048')
        chrome_options.add_argument('--aggressive-cache-discard')
        
        # User Agent ì„¤ì •
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36')
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        print(f"ğŸ”§ ì„ì‹œ ì„¸ì…˜ ë””ë ‰í† ë¦¬: {temp_dir}")
        print(f"ğŸ”§ ë””ë²„ê¹… í¬íŠ¸: {debug_port}")
        
        # Chrome ë“œë¼ì´ë²„ ì´ˆê¸°í™” - Docker í™˜ê²½ì—ì„œëŠ” ì‹œìŠ¤í…œ ChromeDriver ì‚¬ìš©
        driver = None
        try:
            driver = webdriver.Chrome(options=chrome_options)
            print("âœ… Chrome ë“œë¼ì´ë²„ ìƒì„± ì„±ê³µ")
            
        except Exception as e:
            print(f"âŒ Chrome ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}")
            return None
        
        if driver:
            # íƒ€ì„ì•„ì›ƒ ë° ê¸°ë³¸ ì„¤ì •
            driver.implicitly_wait(10)
            driver.set_page_load_timeout(30)
            
            # ì„¸ì…˜ ì •ë³´ ì €ì¥ (ì •ë¦¬ìš©)
            driver._temp_dir = temp_dir
            driver._debug_port = debug_port
            
            # ìë™í™” ê°ì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… Chrome ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            return driver
        else:
            return None
                
    except Exception as e:
        print(f"âŒ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘ ì „ì²´ ì˜¤ë¥˜: {e}")
        return None

def get_webpage_with_selenium(url):
    """Seleniumì„ ì‚¬ìš©í•œ ë™ì  ì›¹í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°"""
    driver = None
    
    try:
        print("Selenium WebDriver ì‹œì‘...")
        
        driver = setup_chrome_driver_ubuntu()
        if not driver:
            print("Chrome ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨")
            return None
        
        print(f"í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
        driver.get(url)
        
        # í˜ì´ì§€ ê¸°ë³¸ ë¡œë“œ ëŒ€ê¸°
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        print("ê¸°ë³¸ í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
        
        # ë™ì  ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
        time.sleep(5)
        
        # í•µì‹¬ ìš”ì†Œë“¤ ëŒ€ê¸°
        for element_id in ["contentBody", "conScroll"]:
            try:
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.ID, element_id))
                )
                print(f"{element_id} ìš”ì†Œ ë¡œë“œ í™•ì¸")
            except:
                print(f"{element_id} ìš”ì†Œ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ")
        
        # pgroup í´ë˜ìŠ¤ ìš”ì†Œë“¤ ëŒ€ê¸°
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, "pgroup"))
            )
            print("pgroup ìš”ì†Œ ë¡œë“œ í™•ì¸")
        except:
            print("pgroup ìš”ì†Œ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ")
        
        time.sleep(3)
        
        html_content = driver.page_source
        driver.quit()
        
        print(f"Seleniumìœ¼ë¡œ ì›¹í˜ì´ì§€ ìš”ì²­ ì„±ê³µ: {len(html_content)}ì")
        return html_content
        
    except Exception as e:
        print(f"Selenium ì›¹í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
        if driver:
            try:
                driver.quit()
            except:
                pass
        return None
    
    finally:
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        if driver and hasattr(driver, '_temp_dir'):
            try:
                import shutil
                shutil.rmtree(driver._temp_dir)
                print(f"ğŸ§¹ ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬: {driver._temp_dir}")
            except:
                pass

def crawl_anticorruption_law():
    """ì²­íƒê¸ˆì§€ë²• í¬ë¡¤ë§ í•¨ìˆ˜"""
    scraped_data = {}
    iframe_url = "https://www.law.go.kr/LSW//lsInfoP.do?lsiSeq=268655&chrClsCd=010202&urlMode=lsInfoP&efYd=20250121&ancYnChk=0"
    
    try:
        print(f"ì²­íƒê¸ˆì§€ë²• iframe í˜ì´ì§€ ì ‘ì†: {iframe_url}")
        
        # Seleniumìœ¼ë¡œ ë™ì  ì½˜í…ì¸  ë¡œë”©
        html_content = get_webpage_with_selenium(iframe_url)
        if not html_content:
            raise Exception("iframe í˜ì´ì§€ HTMLì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        soup = BeautifulSoup(html_content, 'html.parser')
        print("HTML íŒŒì‹± ì™„ë£Œ")
        
        # ë²•ëª… ìˆ˜ì§‘
        try:
            con_top = soup.find('div', id='conTop')
            if con_top and con_top.find('h2'):
                law_name = con_top.find('h2').get_text(strip=True)
                scraped_data['ë²•ëª…'] = law_name
                print(f"ë²•ëª… ìˆ˜ì§‘ ì™„ë£Œ: {law_name}")
            else:
                scraped_data['ë²•ëª…'] = "ë²•ëª… ìˆ˜ì§‘ ì‹¤íŒ¨"
        except Exception as e:
            scraped_data['ë²•ëª…'] = f"ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"
        
        # ì‹œí–‰ ë²•ë¥  ì •ë³´ ìˆ˜ì§‘
        try:
            con_top = soup.find('div', id='conTop')
            if con_top:
                div_elements = con_top.find_all('div', recursive=False)
                if div_elements:
                    law_info = div_elements[0].get_text(strip=True)
                    scraped_data['ì‹œí–‰_ë²•ë¥ _ì •ë³´'] = law_info
                    print(f"ì‹œí–‰ ë²•ë¥  ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {law_info[:100]}...")
                else:
                    scraped_data['ì‹œí–‰_ë²•ë¥ _ì •ë³´'] = "ì‹œí–‰ ë²•ë¥  ì •ë³´ ì—†ìŒ"
            else:
                scraped_data['ì‹œí–‰_ë²•ë¥ _ì •ë³´'] = "conTop div ì—†ìŒ"
        except Exception as e:
            scraped_data['ì‹œí–‰_ë²•ë¥ _ì •ë³´'] = f"ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}"
        
        # pgroup í…ìŠ¤íŠ¸ ì¶”ì¶œ (í•µì‹¬ ê¸°ëŠ¥)
        try:
            # DOM êµ¬ì¡° íƒìƒ‰: bodyId â†’ searchForm â†’ container â†’ center â†’ bodyContentTOP â†’ viewwrapCenter â†’ bodyContent â†’ contentBody â†’ sideCenter â†’ conScroll
            path = [
                ('bodyId', 'body_id'),
                ('searchForm', 'search_form'),
                ('container', 'container'),
                ('center', 'center'),
                ('bodyContentTOP', 'body_content_top'),
                ('viewwrapCenter', 'viewwrap_center'),
                ('bodyContent', 'body_content'),
                ('contentBody', 'content_body'),
                ('sideCenter', 'side_center'),
                ('conScroll', 'con_scroll')
            ]
            
            current_element = soup
            for element_id, var_name in path:
                current_element = current_element.find(id=element_id)
                if current_element:
                    print(f"âœ“ {element_id} ë°œê²¬")
                else:
                    print(f"âœ— {element_id} ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    break
            
            # conScrollì—ì„œ pgroup ìš”ì†Œë“¤ ì¶”ì¶œ
            if current_element:  # conScrollì´ ë°œê²¬ëœ ê²½ìš°
                all_pgroups = current_element.find_all('div', class_='pgroup')
                if all_pgroups:
                    print(f"âœ“ pgroup í´ë˜ìŠ¤ div {len(all_pgroups)}ê°œ ë°œê²¬")
                    
                    pgroup_texts = []
                    for i, pgroup in enumerate(all_pgroups):
                        text = pgroup.get_text(strip=True)
                        if text:
                            pgroup_texts.append(text)
                            print(f"  - pgroup {i+1}: {len(text)}ì")
                    
                    scraped_data['pgroup_í…ìŠ¤íŠ¸'] = pgroup_texts
                    scraped_data['pgroup_ê°œìˆ˜'] = len(pgroup_texts)
                    print(f"ì´ {len(pgroup_texts)}ê°œ pgroup í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    print("pgroup í´ë˜ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            else:
                print("DOM êµ¬ì¡° íƒìƒ‰ ì‹¤íŒ¨")
                
        except Exception as e:
            print(f"pgroup í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
        
        # í¬ë¡¤ë§ ë©”íƒ€ë°ì´í„°
        scraped_data['í¬ë¡¤ë§_ì‹œê°„'] = datetime.now().isoformat()
        scraped_data['ì†ŒìŠ¤_URL'] = iframe_url
        scraped_data['í¬ë¡¤ë§_ìƒíƒœ'] = "ì„±ê³µ"
        
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        scraped_data = {
            'ë²•ëª…': "í¬ë¡¤ë§ ì‹¤íŒ¨",
            'ì‹œí–‰_ë²•ë¥ _ì •ë³´': "í¬ë¡¤ë§ ì‹¤íŒ¨", 
            'í¬ë¡¤ë§_ì‹œê°„': datetime.now().isoformat(),
            'ì†ŒìŠ¤_URL': iframe_url,
            'í¬ë¡¤ë§_ìƒíƒœ': "ì‹¤íŒ¨",
            'ì˜¤ë¥˜_ë©”ì‹œì§€': str(e)
        }
    
    return scraped_data

def save_to_json(data, filename=None):
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    if filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'anticorruption_law_{timestamp}.json'
    
    # crawler_result ë””ë ‰í† ë¦¬ì— ì €ì¥ - Docker ë³¼ë¥¨ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ ì‚¬ìš©
    result_dir = '/home/son/SKN12-FINAL-AIRFLOW/crawler_result'
    
    os.makedirs(result_dir, exist_ok=True)
    filepath = os.path.join(result_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filepath

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ì²­íƒê¸ˆì§€ë²• í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    try:
        data = crawl_anticorruption_law()
        
        if data and data.get('í¬ë¡¤ë§_ìƒíƒœ') == 'ì„±ê³µ':
            filepath = save_to_json(data)
            print("í¬ë¡¤ë§ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
            
            # ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½
            print("\n=== ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ===")
            print(f"ë²•ëª…: {data.get('ë²•ëª…', 'N/A')}")
            print(f"ì‹œí–‰ ì •ë³´: {data.get('ì‹œí–‰_ë²•ë¥ _ì •ë³´', 'N/A')}")
            print(f"pgroup ê°œìˆ˜: {data.get('pgroup_ê°œìˆ˜', 0)}ê°œ")
                
        else:
            print("í¬ë¡¤ë§ ì‹¤íŒ¨")
            print(f"ì˜¤ë¥˜: {data.get('ì˜¤ë¥˜_ë©”ì‹œì§€', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            save_to_json(data, 'anticorruption_law_failed.json')
    
    finally:
        # Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬
        try:
            subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
            subprocess.run(['pkill', '-f', 'chromedriver'], capture_output=True, timeout=5)
            print("ğŸ§¹ Chrome í”„ë¡œì„¸ìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except:
            pass

if __name__ == "__main__":
    main()